---
layout: post
title: "DF Week7 - Optimisation II"
date: 2020-11-13
excerpt: "data fundamentals笔记"
tags: [学习笔记, data fundamentals, 2020, python]
feature: https://miro.medium.com/max/1400/1*gHamsYEMBoVIWbTbn3CQ-A.gif
comments: true
---

[reference: week_7_optimisation_ii.pdf
](https://moodle.gla.ac.uk/pluginfile.php/1241383/mod_folder/content/0/week_7_optimisation_ii.html?forcedownload=1)

infomation is filtered，，已筛选部分用得上的

* 目录
{:toc}

## Outline

* **一阶最优化 first-order optimisation**
* **梯度下降 gradient descent**
* **自动微分 automatic differentiation**
  * 对优化的重要性
* **Jacobian Matrix，gradient vector, Hessian Matrix**
  * 他们如何刻画目标函数的局部行为
* **连续性，利普希茨连续&梯度下降法的关系** continuity, how Lipschitz continuity relates to gradient descent
* **使用随机松弛 - 从不连续目标函数创建平滑连续目标函数** stochastic relaxation used to create smooth continuous objective functions frm discontinuous ones
* **随机梯度下降** stochastic gradient descent
  * 为什么有用?
* **动力，如何改善梯度下降** momentum, how it improve gradient descent
* **二阶方法&局限性** second-order methods & limitations

****

## Example: deep neural networks

![](/static/2020-11-13-21-14-47.png)

* 给定一些观察值`x1...xn`
* 对应输出`y1...yn`
* 找出目标函数? `y'=f(x;θ)`
  * 满足`θ* = argmin ∑i||f(xi;θ)−yi||`
    * 某最优参数时，使得输出`y'`与**预期值`yi`**差异最小?
  * 在如此大的参数空间中，如何能够在合理的时间内进行优化？上面的例子有数千万个需要调整的参数

### 反向传播算法：Backpropagation

设计的 Origami Hummingbird 采用 CC BY 2.0旋转、平移和简单、固定的折叠方式，可以将空间形成任意复杂的形状。答案是，这些网络是以一种非常简单(但聪明的方式)构建的。

传统的神经网络由**一系列层次layers组成**，每一层都是一个**线性映射liner map**(只是一个矩阵乘) ，然后是一个简单的、固定的**非线性函数**。

思考: 旋转，拉伸(线性映射)和折叠(简单的固定非线性折叠)。

一层的输出是下一层的输入

![](/static/2020-11-14-00-26-43.png)
![](/static/2020-11-14-12-44-31.png)

* 每一层由应用到前一层的输入 xx 的线性映射 WW 组成，接着是一个固定的非线性函数 g (x)
* **每一层中的线性映射`W`由一个矩阵（权重矩阵）指定**
  * 网络是通過每層權重矩陣的條目來完全參數化的（這些矩陣的所有條目都可以視為參數向量`θ`）
* 非线性函数 `g (x)`每一层是**固定的，不能变化;**
  * 它往往是一个简单的功能，以某种方式“压缩”输出的范围（如`tanh`，`relu`或者`sigmoid`-你不需要知道这些）
* 在优化过程中，只有权重矩阵会发生变化
  * `yi=G(Wixi+bi)`

🍬 这种特殊结构(在一定条件下)具有一个巨大的优点，即对于网络中的每个权重，即使是多个层次组合在一起，也可以**同时计算出目标函数关于权重的导数**。

* 这个算法被称为**反向传播backpropagation**，它是**自动微分automatic differentiation**的一个算法
* “对权重的导数”意味着我们可以一次性知道每个权重对预测的影响程度，对于整个网络中的每个权重
* <font color="red">如果我们把所有**权矩阵的元素**连接成一个向量 `θ`</font>
  * 我们就可以得到**目标函数 w.r.t `θ` 的梯度**
  * 这使得优化变得“容易” ; 我们只需要沿着我们下坡最快的方向走

### 为什么不用启发式搜索: Why not Heuristic Search

![](/static/2020-11-14-12-55-51.png)

启发式搜索方法，如随机搜索，模拟退火和遗传算法是容易理解，易于实现，并有很少的限制，他们可以应用的问题。那么为什么不总是使用这些方法呢？

* 它们可能非常慢; 可能需要很多次迭代才能达到最小值，并且需要大量的计算才能计算每次迭代
* 没有收敛的保证（真能收敛到一个全局最小值？），甚至没有进步的保证。搜索可能会陷入僵局，或者在高原上缓慢漂移。
* 可以调整的超参数数量巨大(温度时间表、种群大小、内存结构等)。应该如何选择这些？这些参数的最优选择本身就是一个最优化问题

对于像深度神经网络这样的优化问题，启发式搜索是毫无希望的。**在具有数百万个参数的训练网络中取得进展实在太慢了。而是应用一阶优化**

* 今天将要讨论的一阶算法比启发式搜索要快几个数量级

🍬 注意：如果已知**目标函数是有凸约束的凸函数**，则还有更好的优化方法

* 这些方法通常可以在保证收敛的情况下异常快速地运行

## 滚球-物理优化: Phisical Optimisation

可以通过考虑在（光滑）表面上滚动的球来形成高阶优化的直觉，这表示跨2D域的目标函数的值（即，如果我们有一个参数向量  θ  有两个元素）。

![](/static/2020-11-14-13-02-41.png)
![](/static/2020-11-14-13-05-06.png)

重力施加垂直于表面平面的力。曲面沿着曲面法线的方向向球施加力，该向量指向曲面“笔直”的方向

* 这导致在表面最陡的倾斜方向上的分力，使球在该方向上加速。**该梯度矢量始终指向最陡的坡度方向**
* 球最终将稳定在一种配置上，在该配置上施加了一组平衡的力
  * 球最终会在一个受力均衡的形状中稳定下来。例如，如果球沉降到表面法线与重力平行的最小值时，就会发生这种情况

### 吸引子-流向一个解决方案: Attractors: flowing towards a solution

![](/static/2020-11-14-13-05-39.png)

我们可以将其视为吸引子，将我们的搜索引向解决方案。

搜索的轨迹与目标函数的“流场”平行。我们的直觉是，我们可以按照这些流线滚动到“盆地”中找到最小值

## 雅可比-导数矩阵: Jacoian:Matrix of Derivatives

其每一行都是由相应函数的梯度向量转置构成的；

【注】：梯度向量Jacobian矩阵的一个特例；

**当目标函数为标量函数时，Jacobian矩阵是梯度向量**

![](/static/2020-11-14-13-09-05.png)

* 如`f（x）`是标量`x`的标量函数
  * `f'(x)`是`f`的一阶导数，`d/dx f (x)`
  * `f''(x)`是二阶导数，`d2/dx2 f(x)`
* 如果我们将其概括为向量函数 `y =f（x ）` ，
  * **输入的每个分量与输出的每个分量，在任何特定输入之间都有一个变化率（导数）【梯度向量】`X`**
    * 我们可以将此导数信息收集到称为**Jacobian矩阵**的矩阵中

🍊 Jacobian矩阵

* 该矩阵描述**特定点处的斜率 `X`** 如果输入 X ∈R^n, 输出 y ∈R^n ，那么我们有一个  m × n  矩阵
* ![](/static/2020-11-14-13-14-12.png)

🍬 告诉我们，当我们改变input的任何组成部分时，output的每个组成部分改变了多少---- 向量函数的广义“斜率”

* 这是描述**向量函数在 x 点的变化**的一个非常重要的方法
* 在 f 映射 Rn → Rn (从一个向量空间到同一个向量空间)的情况下，我们有一个`n × n`方阵`J`
  * 我们可以用它来做一些标准的事情，比如计算行列式，取特征分解或者(在某些情况下是逆的)

🍬 <font color="red">在许多情况下，我们有一个非常简单的 Jacobian: 只有一行</font>

* 这适用于**一个标量函数 y = f (x)** ，其中 y ∈ r (n 维输入的1维输出)
* 这就是当**损失函数 L (θ)是向量输入的标量函数的情况**。在这种情况下，我们有一个单行的雅可比: **梯度向量**

## 梯度向量-Jacobian一行：Gradient Vector: One row of the Jacobian

![](/static/2020-11-14-13-22-17.png)

* `∇f(x)`是向量的（标量）函数的梯度向量
  * **等效于向量函数的一阶导数**
  * <font color="red">每个`x`的分量都有一个（偏 partial derivative）导数</font>
    * **说明如果独立地对每个维度进行微小改变，`f（x ）`会变化多少**
  * 在本课程中，仅处理具有**向量输入（参数θ），标量输出**的`f(x)`（目标函数）

🍊 归纳

* 如果`L(θ)`是`R^n->R`的映射【向量->标量】
  * 则【一阶导数-梯度向量】`∇L(θ)`是vector valued map `Rn->Rn`
  * <font color="blue">如果`L(θ)`是`Rn->Rn`的映射【向量->向量】，则`∇L(θ)`是matrix valued map `Rn->Rmxn`</font>

`∇L(θ)`是指向`L(θ)`中最陡峭变化方向的向量

## 海森矩阵-梯度向量的雅可比：Hessian：Jacobian of the Gradient Vector

![](/static/2020-11-14-13-51-24.png)

* `∇2f(x)`一个向量的标量函数的Hessian矩阵，等价于**向量函数的二阶导数**
* `∇2L(θ)`是矩阵值映射`Rn->Rnxn`
  * <font color="red">一个标量值函数的二阶导数，随着其输入维度的二次变化而变化</font>
* 如果最初的函数是一个矢量，我们就会得到一个海森张量

### 可微分目标函数：Differentiable objective functions

![](/static/2020-11-14-13-56-25.png)

对于某些目标函数，可以计算**目标函数关于参数 `θ`的(精确)导数**

* 例如，如果目标函数具有单个标量参数 `θ ∈ R`
  * `L(θ) = θ^2`
  * 则根据基本微积分，对 θ 的导数为`L′(θ) = 2θ`【非梯度向量，而是单个标量导数】

🍊 <font color="red">如果我们知道导数，我们可以利用它向“好的方向”移动——**沿着目标函数的斜率[导数]向下移动到最小值**</font>

* 对于**多维目标函数**(`θ`有多个分量) ，这个问题变得稍微复杂一些，
  * 因为**有梯度向量`∇L(θ)`，而不是一个简单的标量导数**。然而，同样的原则也适用

🍊 Orders: zeroth, first, second

**迭代算法可以根据它们需要的导数顺序来分类:**

* **zeroth order零阶优化算法**
  * 计算目标函数`L(θ)`
  * 随机搜索，模拟退火
* **first order一阶优化算法**
  * 计算`L(θ)`&导数`∇L(θ)`
  * 梯度下降法
* **second order二阶优化算法**
  * 计算`L(θ)`&导数`∇L(θ)`&`∇∇L(θ)`
  * 牛顿优化

## 导数优化：Optimisation with Derivatives

![](/static/2020-11-14-14-08-27.png)

如果我们知道(或者可以计算)**一个目标函数的梯度，我们就知道该函数在任何给定点的斜率**, 这给了我们两者:

* **增长最快的方向 direction of dastest increase**
* **斜率的陡度 steppness of that slope**

🍬 <font color="red">知道目标函数的导数就足以显著提高优化的效率</font>

### 条件-可微分：Conditions-Differentiability

一个**光滑函数**具有一定阶数的连续导数。平滑函数通常**更容易进行迭代优化**，因为当前近似值的微小变化可能会导致目标函数的微小变化

* **如果 n 阶导数是连续的，则称一个函数是`Cn` 连续的**

![](/static/2020-11-14-14-12-14.png)
![](/static/2020-11-14-14-12-49.png)

* **一阶优化** - 使用目标函数针对参数的一阶导数,**仅适用于**
  * **至少`C1`连续（一阶导数连续的）**，即函数或其导数没有阶跃变化
  * **可微分的**，即任何地方都有梯度定义

🍬 许多目标函数满足这些条件，一阶方法比零阶方法有效得多。对于特定类型的函数(例如凸函数) ，特定的一阶优化器收敛所需的步数有已知的界限

## 利普希茨连续: Lipschitz continuity (no ankle breaking)

![](/static/2020-11-14-14-19-18.png)

**一阶(和高阶)【连续优化】算法的条件**

* 要求`C1`连续【1.可微分】，**并且要求函数是 Lipschitz 2.利普希茨连续的**
* 对于目标函数`L(θ)`，`Rn->R`
  * 相当于说，斜率被限制【有最大陡度】，**函数改变（斜率）不能比一个常量`K`更快**

### 利普希茨常数

![](/static/2020-11-14-14-23-49.png)

我们可以想象在一个表面上运行一个特定陡度的圆锥。我们可以检查它是否接触到表面。

这是对函数陡峭程度的度量，或者**等价于一阶导数的界**

函数`f(x)`的Lipschitz 常数 `k` 是这个锥的宽度的度量，这个锥只接触函数一次。这是对函数平滑程度的度量，或者**等价于目标函数在其域上任意点的最大陡度**。它可以被定义为:

* ![](/static/2020-11-14-14-27-01.png)
* `sup` - supermum上确界
  * 大于f(x)每个值的最小值
* **更小的`k`表示更平滑的函数**
  * `k = 0`是完全平坦的。
  * 我们将假设我们将要处理的函数有一些有限的 k，尽管它的值不能精确地知道

## 解析微分法: Analytical Derivatives

![](/static/2020-11-14-14-31-28.png)
![](/static/2020-11-14-14-34-48.png)

* 数学形式直接计算导数（如果知道函数在封闭形式下的导数）
* 计算导数
* 计算导数等于0的情况，`f'=0`
  * 找出fx的所有转折点 & 最优状态
* 检查是否所有解都有正二阶导数`f''>0`
  * 验证是否解有最小值

🍊 例子

![](/static/2020-11-14-14-35-22.png)
![](/static/2020-11-14-14-35-53.png)

### 梯度-导数向量：Gradient: A Derivative Vector

![](/static/2020-11-14-14-37-04.png)

* 使用目标函数-接收vector，返回scalar
* 数学形式表示向量`θ`导数
  * ![](/static/2020-11-14-14-39-00.png)
* `∇L(θ)`梯度，梯度向量
  * 在任何给定的点上，函数的梯度**指向函数增长最快的方向**
  * 梯度向量的大小是**函数变化的速率(“陡度”)**

## 梯度下降法(基本一阶优化)：Gradient Descent

![](/static/2020-11-14-14-41-11.png)

基本的一阶优化算法，称为梯度下降法

* 从初始值`θ^(0)`initial guess开始
* ![](/static/2020-11-14-14-42-47.png)
  * `δ` - **缩放超参数，步长**
    * 步长可以固定，也可以根据线搜索等算法，自适应选择

🍊 优化器，算法需要在目标函数下降最快的地方采取行动，步骤即

* 在某参数开始`θ(0)`
* 重复
  * 检查每个方向的陡度,即某参数在每个方向上的陡度 `v=∇L(θ^i)`
  * 朝更抖方向`v`移动步长`δ`，找到下一个参数`θ(i+1)`

🍬 `θ(i)`表示迭代序列中的第`i`次

### 下山并不总是最短的路线: Downhill is not always the shortest route

![](/static/2020-11-14-14-55-30.png)

虽然梯度下降法可以非常快，**但是沿着坡度走并不一定是最快到达最低点的路线**。在下面的例子中，从红点到最小值的路线非常短。然而，遵循这个梯度，需要一个非常迂回的路径到最小值

图片: 梯度下降法意味着走最陡的斜坡---- 并不总是最短的路线

* 不过，这通常比盲目地跳来跳去希望到达底部要快得多

### 实现梯度下降法：Implementing Gradient Descent

收敛于19步
![](/static/2020-11-14-14-57-21.png)

🍊 Why step size matters 为什么步长很重要

![](/static/2020-11-14-15-02-18.png)
![](/static/2020-11-14-15-02-28.png)
![](/static/2020-11-14-15-02-38.png)
![](/static/2020-11-14-15-03-00.png)

* 步长 δ 是梯度下降成功的关键。
  * 如果δ太小，步长就会非常小，收敛速度也会很慢
  * 如果δ太大，优化的行为可能变得相当不可预测
  * 如果梯度函数在一个步骤的空间中发生了显著的变化(例如，如果梯度在整个步骤中改变了符号，正负？) ，就会发生这种情况

### 与利普希茨常数的关系：Relationship to Lipschitz constant

![](/static/2020-11-14-15-04-18.png)

我们不会证明这一点，但是**最佳步长 δ 与目标函数的 Lipschitz 常数 k 直接相关**。

* 不幸的是，在许多现实世界的优化任务中，我们很少精确地知道`k`
* **而且步长`δ`通常是由类似线性搜索的近似方法设置的**

### 2D梯度下降：Gradient descent in 2D

![](/static/2020-11-14-15-06-41.png)

只要能得到参数空间中任意点的**梯度向量**，这种方法就可以扩展到任意维数。

* 我们有一个梯度向量 `∇L(θ) `，而不是一个简单的一维导数。代码没有任何更改

### 目标函数的梯度：Gradients of the objective function

![](/static/2020-11-14-15-08-23.png)

**为了使一阶优化成为可能，目标函数必须有导数**

* 这显然不直接适用于经验优化(例如，现实世界的制造业中，零部件的质量在实验中被测试---- 没有导数)
* 但它可以应用于许多情况下，我们有一个计算模型，可以优化。这也是在优化时支持构建模型的一个原因

### 为什么不用数值差异：Why not use numerical differences

![](/static/2020-11-14-15-10-38.png)
![](/static/2020-11-14-15-14-16.png)

* 为什么不使用**数值微分 numerical differentiation**方法，计算L(θ)的导数？
  * 对相当平滑的一维函数来说，非常有效
  * 这些是有限的差异???

#### 数值问题：Numerical Problems

![](/static/2020-11-14-15-15-25.png)

选择 h 也是困难的，这样函数就不会被过多的值错误表示，但是数值问题不会支配结果。记住，有限差分违反了优秀浮点结果的所有规则: f (x + h)-f (x-h)2h f (x + h)-f (x-h)2h

* 往更大数字`x`中添加小`h`
  * maginitude error
* 减去两个相似的数
* 将结果/很小的数`2h`
* 很难想象一个简单的例子，有更多的潜在数值问题比有限的差异！

#### 维数灾难的复仇：Revenge of the curse of dimensionality

![](/static/2020-11-14-15-20-39.png)

然而，即使我们能够处理数字问题，这在高维空间中也是没有用的。维数灾难再次发动袭击。

为了计算 `x` 点的梯度，我们需要计算每个维度`xi`上的数值差异。

如果`θ`有一百万个维度，那么每个单独的导数求值将需要计算两百万个`l (θ)`

* 零阶一阶方法的加速度会被梯度的计算所淹没

## Improving gradient descent 改善梯度下降法

梯度下降法方法可以是非常有效的，而且通常比零阶方法要好得多，但是它**也有缺点**

![](/static/2020-11-14-15-27-59.png)

* 损失函数的斜率`L′(θ)=∇L(θ)`，必须在特定点算出
  * **使用自动微分 automatic differentiation**
* **梯度下降法可能会被困在当地的极小地方。这是梯度下降法方法固有的一面**
  * 除非函数是凸的，步长是最优的，否则它不会找到全局最小值。
  * **随机重新启动和动量 Random restart & momentum**可以降低对局部极小值的敏感性
* **梯度下降法只对平滑的、可微的目标函数有效**
  * **随机松弛 Stochastic relaxation**引入随机性，允许非常陡峭（非平滑）的函数进行优化
* 如果目标函数(和/或梯度)的计算速度很慢，**梯度下降法可能会非常慢**
  * 如果目标函数可以写成许多子问题的简单总和，那么**随机梯度下降 Stochastic gradient descent**可以极大地加速优化

### 自动微分：Automatic differentiation

![](/static/2020-11-14-15-38-18.png)

如果我们解析地知道目标函数在封闭形式下的导数，这个问题就可以得到解决。例如，我们在上节课中看到的最小二乘线性回归的导数，相对来说很容易精确地计算成一个公式。然而，手工计算目标函数的导数似乎非常困难，对于一个复杂的多维问题来说，这可能是非常复杂的。这是算法差异化的主要动机(或自动微分)

自动微分可以取一个函数，通常是一个完整编程语言的子集，**然后自动构造一个函数，在任意给定点求精确的导数**。这使得执行一阶优化是可行的

![](/static/2020-11-14-15-39-53.png)
![](/static/2020-11-14-15-40-07.png)
![](/static/2020-11-14-15-40-30.png)
![](/static/2020-11-14-15-42-06.png)
![](/static/2020-11-14-15-42-36.png)
![](/static/2020-11-14-15-43-09.png)
![](/static/2020-11-14-15-43-26.png)
![](/static/2020-11-14-15-43-39.png)

### 自动微分的极限：Limits of automatic differentiation

计算多维函数的二阶导数变得非常困难

### 随机松弛：Stochastic Relaxation

![](/static/2020-11-14-15-51-01.png)

他们的论点是，尽管每一个特定的情况都是简单的二元选择，但它是在许多随机情况下的平均值，在这些情况下，条件会略有不同(可能是接近黑暗，可能是捕食者眼睛不好，可能是天气不好) ，在所有这些情况下，**平均值的一些非常微小的色彩变化可能会提供一个优势。从数学上讲，这是随机松弛; 通过积分许多不同的随机条件，表面上不可能的陡峭梯度已经(近似地)变成了 Lipschitz 连续**

* 一个非常陡峭的函数在某一点有一个非常大的导数;
  * 或者它在某些部分可能有零导数。但是如果我们对很多情况下，步长位置稍微移动的情况，我们得到一个平滑的函数

### 随机梯度下降：Stochastic Gradient Descent

![](/static/2020-11-14-15-53-17.png)

在每次迭代之前，梯度下降法评估目标函数及其梯度。这样做可能非常昂贵，特别是在用大型数据集(例如机器学习)逼近优化函数时

* 如果**目标函数可以分解成小部分**
  * 优化器可以独立地对**随机选择的部分进行梯度下降法优化**
  * 这可能会快得多。这就是所谓的随机梯度下降，因为<font color="red">它所采取的步骤取决于目标函数部分的随机选择</font>

![](/static/2020-11-14-15-58-54.png)

![](/static/2020-11-14-16-03-02.png)

* 逼近问题中，有训练集`xi`，已知输出`yi`
  * 找到参数`θ`，**使得模型输出&预期输出的差异最小**

#### 内存优势：Memory advantages

![](/static/2020-11-14-16-06-00.png)

#### 启发式增强：Heuristic enhancement

![](/static/2020-11-14-16-07-34.png)
![](/static/2020-11-14-16-07-25.png)

虽然**添加噪音是一种启发式搜索方法**(不能保证它会让事情变得更好，甚至不会让事情变得更糟) ，但它通常非常有效。

我们基本上得到了一种有限形式的随机松弛的好处——我们的目标函数可以通过对随机子样本的平均来“平滑” ，所以即使它不完全是 Lipschitz 连续(或者有一个坏的 Lipschitz 常数) ，SGD 也可以工作得很好

#### 使用SGD

![](/static/2020-11-14-16-08-39.png)

不能保证SGD甚至会朝着正确的方向前进。在实践中，它对于许多现实世界的问题是非常有效的。例如，让我们再次考虑将一条直线拟合到观测值的问题(线性回归)。

![](/static/2020-11-14-16-10-10.png)

与计算整个和不同，我们可以计算一个随机子集上的梯度，然后用它来计算下一步。

### 随机梯度下降-线性回归：Linear regression with SGD

![](/static/2020-11-14-16-10-56.png)
![](/static/2020-11-14-16-11-48.png)

我们可以用10000点来做我们的线性回归数据库的例子，并且只需要通过一次数据就可以找到一个很好的匹配。**这是因为我们可以把问题分解成许多小问题(一次在几个随机点上拟合一条直线)**，这些问题都是一个大问题的一部分(拟合一条直线到所有点)。

当这是可能的，它是远远更有效的计算梯度为整个集合的数据，我们可用。

## Nightmare Function

![](/static/2020-11-14-16-12-32.png)
![](/static/2020-11-14-16-13-06.png)

目标函数无法分解成简单的子目标函数，SGD也不能优化

### 随机重启：Random restart

![](/static/2020-11-14-16-16-45.png)

* 启发式方法
  * 运行梯度下降，直到卡住
  * 随即**重新启动不同初始条件random restart**，再重复尝试

### 简单记忆-动量项：Simple memory: momentum terms

![](/static/2020-11-14-16-17-48.png)
![](/static/2020-11-14-16-30-26.png)

物理球在表面上滚动并不会因为表面的一点不规则而停止。一旦它开始滚下山坡，它可以跳过小小的凸起和草皮，穿过平坦的平原，头稳定地沿着狭窄的山谷而不会被边缘反弹。**这是因为它有动量，它会倾向于保持原来的方向。这是内存元启发式的一种形式**。优化器只记住一条简单的路径，而不是到处都有类似蚁群的路径。

同样的思想也可以用来**减少【随机的】梯度下降法因为目标函数的小波动而陷入困境的机会**，并且“平滑”下降。

* 这个想法很简单，如果你现在走的是正确的路线，那么就继续走下去，即使坡度并不总是向下
* **引入速度`v`，让优化器朝这个方向移动**
  * **逐渐调整`v`，使其与导数对齐**
  * ![](/static/2020-11-14-16-21-25.png)
  * **由参数`α`决定，`α`越接近`1.0`，动量越大**
  * **`α=0.0`，相当于普通梯度下降**

## 临界点类型：Types of critical points

图为临界点的分类，每一个对应Hessian特征值的不同配置

![](/static/2020-11-14-16-31-02.png)

这种物理的表面直觉引导我们思考如何描述一个目标函数的不同部分。

对于光滑连续的目标函数，在空间中有各种特殊的点。这些是临界点，其中梯度向量分量是零向量

## 二阶导数：Second-order Derivatives

![](/static/2020-11-14-16-33-15.png)

如果一阶导数表示函数的“**斜率slope**” ，二阶导数表示函数的“**曲率curvature**”

* 对于每个参数分量`θi`，Hessian存储了其他`θj`的陡度如何变化

🍊 想象在一座山上

* 所处高度 - 目标函数的值
* 可以改变的参数 - 位置，东南西北
* 梯度向量 - 向北一步/东一步时高度的变化。2个参数，是在山上所处地方的局部陡度
* Hessian存储了，我向北走时山越来越陡峭，向东走时山越来越陡峭; 向东走时也是如此。所以有一个**2x2的矩阵来描述这些陡度的变化**

![](/static/2020-11-14-16-37-57.png)

* Hessian中每一条，都是function中的一个维度
  * 您可能会注意到它与协方差矩阵的相似之处
  * **协方差矩阵 - 数据坐标如何相互变化**
  * **Hessian - 捕获了函数的梯度如何相互变化**

🍊 对于二维曲面，梯度向量指定在给定点与曲面相切的平面的法线。Hessian矩阵指定一个二次函数(一个光滑曲线与一个最小值)跟随曲面的曲率

## Hessian特征值：Eigenvalues of Hessian

![](/static/2020-11-14-16-43-31.png)
![](/static/2020-11-14-16-48-16.png)

Hessian矩阵抓住了关于**临界点类型**的重要性质，特别是，**Hessian的特征值告诉有什么样的点**

* 如果所有的特征值都是严格正定的，则称该矩阵为**正定postive definite**，点为最小点
* 如果所有的特征值都是严格负的**负定的)(negative definite)** ，且点是一个极大值
* 如果特征值具有混合符号，则该点为**鞍点saddle point**
* 如果特征值都是正/负的，但有一些零，则矩阵是**半定semidefinite**的，点是**坪/岭 plateau/ridge**

## 二阶优化：Second-order Optimisation

![](/static/2020-11-14-16-48-50.png)

二阶优化使用 Hessian 矩阵，**在一个步骤中跳到每个局部二次近似的底部**

* 它可以跳过平坦的plains，也可以从马鞍点saddle point逃离
* 这样可以减慢梯度下降法的速度
* **二阶优化方法通常比一阶方法快得多**

### 维度灾难：Curse of Dimensionality

![](/static/2020-11-14-16-51-08.png)

然而，简单的二阶方法并不适用于高维情况。计算 Hessian 矩阵需要 `d2`计算和`d2`存储。许多机器学习应用程序都有 `d > 1` 100万个参数的模型。仅仅存储一次优化迭代所需Hessian矩阵就需要:

二阶优化可以比一阶优化方法(如梯度下降法)更快地通过鞍点和高原

* 它在低维问题上特别有效，但是维数灾难总是有方法出现
* 假设我们有一个100万参数的问题需要优化。梯度向量的元素与参数向量的元素数量一样多; 因此，float64需要100万个元素，或大约8兆字节的内存需求。但是Hessian矩阵必须在每一对参数中存储变化。这需要:
  * 8000000000000 bytes of memory
  * 也就是8tb！这是一个不可能的计算负担。二阶方法有一些特殊的、有限存储的形式，它们使用近似的Hessian算法(如广泛使用的 L-BFGS 算法) ，但这些都超出了本课程的范围

