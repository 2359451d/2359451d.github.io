---
layout: post
title: "DF Week4 - Computational Linear Algebra I"
date: 2020-10-18
excerpt: "data fundamentals笔记"
tags: [学习笔记, data fundamentals, 2020, python]
feature: https://miro.medium.com/max/1400/1*gHamsYEMBoVIWbTbn3CQ-A.gif
comments: true
---

[reference: week_4_Computational Linear Algebra I.html
](https://moodle.gla.ac.uk/pluginfile.php/1241379/mod_folder/content/0/week_4_matrices_i.pdf?forcedownload=1)

infomation is filtered，，已筛选部分用得上的

* 目录
{:toc}

## Outline

you should know

* **vector & vector space**
  * standard operations on vectors: add & multiplication
* **norm 范式**
  * how it can be used to measure vectors 如何使用norm测量vector
* **inner product 向量内积**
  * 向量内积如何产生向量的几何形
* **mathematical vectors** map onto **numerical arrays** 数学矢量如何映射到数字数组上
* 不同的**p规范(p-norm**)及其用法
* 向量表示的重要计算用途
* how to **characterise vector data with a mean vector and a covariance matrix** 如何用平均矢量和协方差矩阵表示矢量数据
* properties of high-dimensional vector spaces **高维向量空间的性质**
* basic notation for matrices 矩阵的基本符号
* view of matrices as linear maps 矩阵视图 - 线性映射
* how basic geometric transforms are implemented using matrices 使用**矩阵实现基本的几何变换**
* how matrix multiplication is defined and its algebraic properties 定义**矩阵乘法及其代数性质**
* the basic anatomy of matrices **矩阵的基本组成**

## Example: Text

string-弱结构，无法贡献于机器翻译系统

* 可以给文本片段**添加额外数学结构**
  * **将文本片段放置在向量空间(vector space)中**
  * **片段可以是单词、部分词或整个句子。这就是所谓的嵌入**算法Word2vec可以学习从字符串到高维向量(通常大于100D)的转换，只需观察大量的文本
* **虽然向量空间每个维度没有特定意义，但是嵌入embedding意味着语义semantics映射到空间关系上**

## Vector spaces

本课考虑向量是实数有序元组

* `[ x1，x2，... xn ] ，xi ∈ r`
* 向量固定维度`n`
  * **元组长度 length of the tuple**
* 可以假设向量中**每个元素**都为**与其他元素正交方向上的距离 a distance in an direction orthogonal**

🍊 例子 - 有需要记的概念

* **长度为3的向量（3维向量）**，可以用于表示笛卡尔坐标系中的空间位置
  * 每个向量有3个正交的测量量 3 **orthogonal measurements** for each vector
  * orthogonal - independent **正交即独立，即，=90°**
* 3D vector`[5,7,3]`
  * `ℝ^3`
  * 组成 `5*[1,0,0] + 7*[0,1,0] + 3*[0,0,1]`
  * 可看为**正交单位向量的加权和 weighted sum of orthogonal unit vectors**
  * 该向量空间有3个独立basis vectors, 因此为三维（向量长度也为3）
* `[1,0,0], [0,1,0], [0,0,1]`
  * **unit vector (basis vectors)**
  * 每个向量，长度为`1`
  * 都指向一个单独的方向(正交方向) orthogonal direction

🍊 向量表示

![](/static/2020-10-28-01-02-12.png)

* 用**小写加粗字母**表示向量

## Points in Space

### Notation

标记`ℝ^𝑛`

![](/static/2020-10-28-01-05-43.png)

* `ℝ`实数集
* `ℝ≥0`非负实数集合
* `ℝ^n`N 个实数的元组的集合 the set of tuples of exactly `n` real numbers
* `ℝ^n*m` n行m个元素的2维实数矩阵集合
* `(ℝ^𝑛,ℝ^𝑛)→ℝ` map：一对n维向量-实数的映射对
  * a map from a pair of n-dimensional vectors to a real number

## Vector spaces

![](/static/2020-10-28-01-16-47.png)

* 任何**规定了维数`n`的向量都位于【向量空间 `ℝ^𝑛`】**
  * 目前向量空间只考虑有限维度的实数向量空间（标准单位基本向量）【还有复数，无限维度空间】
  * **该向量属于，任何长度为`n`的，元素为实数的向量集合中**

🍊 且在该向量空间`ℝ^𝑛`下，规定了如下操作

* **标量乘法/标量积 `ax`** scalar multiplication
  * 对于任何标量`a`，都定义了`ax`（标量积？）
  * 对于实数向量，`ax=[ax1,ax2,...,axn]`元素缩放 elementwise scaling
  * ![](/static/2020-10-28-01-29-19.png)
* **向量加法 `x+y`** vector addition
  * `x+y`个向量， `x`& `y`维数相同
  * **实数向量**，`𝐱+𝐲=[𝑥1+𝑦1,𝑥2+𝑦2,…𝑥𝑑+𝑦𝑑]`元素和
  * ![](/static/2020-10-28-01-32-07.png)
* **norm `||𝐱||` 范式**
  * 测量向量长度
  * `ℝ𝑛→ℝ≥0`
  * ![](/static/2020-10-28-02-32-52.png)
* **向量内积 `⟨𝐱|𝐲⟩  or 𝐱∙𝐲`** inner product
  * **可以比较2个向量夹角 angle**
  * <font color="red">2个正交向量的向量内积为`0`，angle=90</font>
  * **实数向量的向量内积，即dot product点积** `𝐱∙𝐲=𝑥1𝑦1+𝑥2𝑦2+𝑥3𝑦3…𝑥𝑑𝑦𝑑`
  * ![](/static/2020-10-28-02-33-09.png)

🍬 向量间所有操作都在同一向量空间中定义

* 不同维度的向量，不属于同一向量空间，不能进行如上定义的操作

### topological & inner product spaces

拓扑&内积空间

![](/static/2020-10-28-02-41-33.png)

* 有了范式norm， 向量空间就是**拓扑向量空间**，意味着
  * 向量空间是连续的，且向量“紧靠在一起”（或向量周围有领域neighbourhood）
  * 对于向量内积，**向量空间即内积空间（内积集合？）inner product space**

🍊 空间中的向量点vector points、箭头指向，是从原点还是数字元组？

![](/static/2020-10-28-02-47-06.png)

* 数字元组？**或想象这些向量点在向量空间中**
* 向量表示数据，数据在向量空间中
  * vectors to represent data; data lies in space
* 矩阵由对数据的操作得来，矩阵扭曲向量空间
  * matrices to represent operations on data; matrices warp space

### Relation to arrays

与矩阵关系

* 实数向量可以用一维浮点数组来表示，在本系列的第一讲中，我们称之为“向量”
* 但是要小心，**表示和数学元素是不同的东西**，就像浮点数不是实数一样

![](/static/2020-10-28-03-04-57.png)
![](/static/2020-10-28-03-05-55.png)

标量积
![](/static/2020-10-28-03-06-21.png)
![](/static/2020-10-28-03-06-27.png)

![](/static/2020-10-28-03-08-18.png)
![](/static/2020-10-28-03-08-52.png)

* 向量相加
  * `x+y`
* 范式
  * `np.linalg.norm(x)`
* 点积，向量内积
  * `np.dot(x,y)`

### uses of vectors

向量使用

* **composed**
  * addition
* **compared**
  * norms/inner products
* **weighted**
  * scaling

🍬 向量可以对数据进行各种变换

* 因此vector->ndarray映射，可以更高效简洁操作

### Vector Data

数据集dataset，通常以**2D表格**存储

![](/static/2020-10-28-20-11-28.png)

* 可以看为向量列表 lists of vectors
* 每行 - 代表“observation”的vector
  * 每条观察记录stacked up组成2D矩阵
* 每列 - 跨多条观察值的一种元素

🍊

* 以上例子，每行可看为一个在向量空间`ℝ^4`中的向量
* 以上整个矩阵是同一个向量空间中的**向量序列**。这意味着我们可以对表格数据进行**几何描述 geometric**

### Geometric Operations

矢量最明显用途

* 表示二维/三维几何数据
* 游戏or3D渲染引擎中所有计算都由大规模重复**低维向量操作（2D,3D,4D）**组成

![](/static/2020-10-28-20-35-25.png)

图形化管道将所有东西(空间位置、表面法线方向、纹理坐标、颜色等)都处理为大量的矢量数组Graphical pipelines process everything (spatial position, surface normal direction, texture coordinates, colours, and so on) as large arrays of vectors

* 在 GPU 上进行图形编程主要涉及将**数据打包到低维向量数组**(在 CPU 上) ，然后使用**着色器语言在 GPU 上快速处理它们** Programming for graphics on GPUs largely involves packing data into a low-dimensional vector arrays (on the CPU) then processing them quickly on the GPU using a **shader language**

### Machine Learning Applications

机器学习很大程度上依赖向量表示

![](/static/2020-10-28-21-24-33.png)

* 将一些数据转换成**特征向量feature vectors**
  * 特征向量：向量空间中数据编码（如表格数据）
  * describe some data point that might be a vector representing a word image
* 创建一个函数，将**特征向量转换**为预测prediction(例如类标签)
  * 特征转换：处理raw data, 输出特征向量

🍊 最简单有效的机器学习算法之一

> k近邻算法：给定一个训练集，新输入实例。**在训练集中找到与该实例最邻近的k个实例**。
>
> 这**k个实例多数属于某个类**，就把该新输入实例分类到这个类中

* **k nearest neighbours**k近邻算法
  * 涉及data训练集，包含`xi,yi`对
    * 特征向量`xi`，标签`yi`
* 当一个新特征需要分类进行预测时，**计算该训练集中的 k 个最近的向量，使用范数计算距离**
  * 输出预测（把新输入归到多数类中）是这些 k 近邻实例中**出现次数最多的类标签**(k 在某种程度上是预设值; 对于许多问题，它可能在3-12左右)

### Image Compression

![](/static/2020-10-28-22-52-07.png)
![](/static/2020-10-28-22-54-56.png)

* 8x8 image patch could be unraveled to a 64-d vector
  * these vectors could be treated as elements of a vetor space

## Basic Vector Operations

### Add & Multiply

Elementwise addition & scalar multiplication 对位相加 & 标量积 ---【weighted sum】

![](/static/2020-10-29-13-36-50.png)
![](/static/2020-10-29-14-44-27.png)

* 形成**weighted sum** of vectors
* `𝜆1𝐱1+𝜆2𝐱2+⋯+𝜆𝑛𝐱𝐧`
* only applies to vectors of the same dimension

🍊 因为defined addtion & scalar multiplication

![](/static/2020-10-29-14-38-26.png)
(10 subdivisons of the line, find intermediate points accrording to linear interpolating---因此通过线性插值，可以找到**两向量间的中间点向量**，等)

* 可以应用标准统计量&操作
* 如，两向量的**linearly interpolate 线性插值**
  * 两值的线性插值由**参数`α`**控制
  * `lerp(𝐱,𝐲,𝛼)=(1−𝛼)𝐱+(𝛼)𝐲`
    * `(1-α)`scaling factor
  * **线性插值：沿着两个向量之间的直线移动: 当 α 从0到1时，得到结果是从 x 到 y 的一条平滑的直线** --- find a new vector between the line x~y

### 向量欧几里得长度(范数): How big is the vector

可以考虑给向量空间**定义距离 distance**

🍊 向量的**欧几里得长度/范数（向量大小）**（Euclidean length/norm of vectors）

![](/static/2020-10-29-15-11-08.png)
![](/static/2020-10-29-15-06-53.png)

* `||𝐱||`
  * 向量长度表示向量大小？--单个向量的范数=其各维平方和,的平方根
  * 或是两向量之间相差的距离，可以找到最相似的
* `np.linalg.norm()`

🍊 球体内向量距离 -半径？

![](/static/2020-10-29-15-11-18.png)

### 两向量距离&lp/minkowski范数：Different Norms

默认范数

![](/static/2020-10-29-15-23-07.png)

* **欧几里得范数/标准 Euclidean norm/distance measure**
* **包含有欧几里得范数的实数向量的向量空间称为---欧几里得空间**
* **两向量间的距离 --- 两向量间差的范数** distance between 2 vectors - norm of the difference of 2 vectors 
  * `||𝐱−𝐲||2`

🍊 也有其他方法**测量一个单向量的范数/长度** (都是欧几里得范数的延申)

![](/static/2020-10-29-15-21-49.png)
![](/static/2020-10-29-15-44-43.png)
![](/static/2020-10-29-15-50-16.png)
![](/static/2020-10-29-21-04-36.png)

* <font color="red">某些特定上下文中，适用</font>
* **Lp-norms**
  * `‖𝐱‖𝑝`
  * 是一组范数
  * `xi^p` - element, to the power `p`
  * **向量`x`中每个元素`xi`取`p`次方，求和。最后开`p`次方根**
  * <font color="red">控制模型复杂度减少过拟合。一般在损失函数中加入惩罚项</font>
* **`p`具体看是L1还是L2(欧几里得)还是其他范数**
  * **L0范数 - 表示向量x中非0元素的个数**![](/static/2020-10-29-23-00-16.png)
  * **L1范数 - 等于向量中所有元素绝对值之和**![](/static/2020-10-29-23-01-14.png)
  * **L2范数 - 范数表示向量（或矩阵）的元素平方和（再开根）**![](/static/2020-10-29-23-01-46.png)
* L∞可以近似看为`(xj^∞)^1/∞`，maximum
  * +∞ maximum
  * -∞ minimum
* **Minkowski norms 闽可斯基距离**

🍊 不同范数&上下文应用

![](/static/2020-10-29-15-50-48.png)
![](/static/2020-11-08-16-02-02.png)

* 向量
  * L1 -所有元素绝对值的和
  * L2 - 所有元素平方和，最后开根
  * Linf - 最大元素和
* 矩阵
  * 需要看axis，

### Unit Vectors & normalisation

单位向量

![](/static/2020-10-29-22-49-56.png)(length 1 from the origin)
![](/static/2020-10-29-22-50-07.png)

* 范数为`1`
  * **取决于选用的范数**
* unit vector 总指向欧几里得范数为`1`的向量

#### 标准化/归一化：normalisation

归一化/标准化（欧几里得范数）

* **将向量`x`缩放`1/||x2||`倍?**
  * `||x2||` - **L2 norm, Euclidean norm**
* 向量坐标都除于向量的长度
  * <font color="red">新向量的长度恰好为1</font>

```python
x = np.random.normal(0,5,(5,))#random x vector
x_norm = x/np.linalg.norm(x) # x norm after nomalisation( length of 1)

```

### Inner Products: dot product

向量内积（点积）

![](/static/2020-10-29-23-37-39.png)

* 结果为标量IR
* 可求两向量间夹角
* `x·y = ||x||||y||cosθ`
* `x·y = x1*y1 + x2*y2 +...+xiyi`
  * 每维元素对位相乘的连加和
* <font color="red">非单位向量的点积，可以先标准化？</font>
* <font color="red">两个平行单位向量，点积为1</font>
  * cosθ=0=x*y

```python
np.inner(x,y) # inner product of x and y
```

🍊 内积可用于比较不同大小的向量

* 因为只依赖于向量方向
* 如，它被广泛应用于信息检索
  * 比较文档向量document vectors，这些文档向量将文档中的术语表示为大or稀疏的向量
  * 对于长度不同的文档，这些向量的大小可能存在很大差异

## Basic Vector statistics

向量可有其他操作（统计量相关）

![](/static/2020-10-29-23-56-19.png)

* 只需应用加法，标量积，外积等定义

### Mean vector

🍬 center a vector是一种常见标准化方法？

* 用于arrange things
  * average position of a collection of actions at 0

#### Center a dataset

🍊 平均向量 & center a dataset

![](/static/2020-10-30-00-16-06.png)

* 一组`N`个向量的mean vector是这些向量的和乘`1/N`
* 是一组矢量的几何质心 geometric centroid of a set of vectors【**average center point**】
* **如将多个向量叠加(stack)进一个矩阵中，每行一个向量**
  * **`np.mean(x,axis=0)`**
  * 跨行压缩，求mean
  * 我们可以将**存储为向量数组的数据集**中心为零，只需要从每一行中**减去平均向量** **center a dataset**(stored like vector array) to 0 mean
    * <font color="red">移动向量，每个向量都不包含平均偏移，位于origin。之后mean vector为`0`</font>

```python
x= np.random.normal(2,1,(30,4)) # 20 rows, 4 columns

mu = np.mean(x, axis=0)#mean vector
# or manually np.sum(x, axis=0)/x.shape[0]

"""
center a dataset
"""
x_center = x-mu# center the dataset(vectors array)
mu_c = np.mean(x_center, axis=0)# now the mean vector is 0
```

## High-Dimensional Vector Spaces

![](/static/2020-10-30-00-22-48.png)

低维空间中的向量，如2D 和3D，在操作中是很常见的。然而，数据科学常常涉及到高维向量空间，它们遵循我们已经定义的相同的数学规则，但是它们的性质有时是不直观的。

机器学习、优化和统计建模中的许多问题涉及到使用许多测量，每个测量都有一个简单的本质; 例如，一幅图像只是一系列亮度测量。一个512x512的图像可以被认为是由262144个元素组成的单一矢量。**我们可以考虑一个“数据点”作为测量的向量**。这些“特征矢量”的维数 d 对算法的性能和行为有着巨大的影响，而建模中的许多问题都与处理高维空间有关

* 特征向量（本征向量）是一个非简并的向量，其方向在该变换下不变。该向量在此变换下缩放的比例称为其特征值（本征值）
* **指在变换下方向不变，或者简单地乘以一个缩放因子的非零向量**
* **特征向量对应的特征值是它所乘的那个缩放因子**
* **特征空间就是由所有有着相同特征值的特征向量组成的空间，还包括零向量，但要注意零向量本身不是特征向量**

高维可以表示任何 d维数 > 3;
20维特征集可以称为中维;
1000维可以称为高维;
1m 维数据集可以称为极高维。这些都是宽松的术语，因学科不同而有所不同

* **在高维空间中存在大量的空白空间，而且在数据稀疏的地方，很难在高维空间中进行概括**

### Curse of dimensionality

维数灾难

**许多在低维情况下运行良好的算法在高维情况下会崩溃**。这个问题在数据科学中是普遍存在的，被称为维数灾难。了解维数灾难数据库对于任何数据科学都是至关重要的

* Curse of dimensionality: as dimension increases generalisation gets harder exponentially **维数灾难: 随着维度的增加，概括变得越来越困难**

> Many algorithms that work really well in low dimensions break down in higher dimensions. This problem is universal in data science and is called the curse of dimensionality. Understanding the curse of dimensionality is critical to doing any kind of data science.

![](/static/2020-10-30-00-28-48.png)

* 2Dvector中【两种测量量 two different measurements: temp & humidity】，每维20个分组
  * 一共400个分组
  * **但是测量量不够填充所有分组**
    * most bins are empty & a few are heavily populated

🍬 如果维数更高

* 10个不同测量量
  * 空气温度、空气湿度、纬度、经度、风速、风向、降水量、一天的时间、太阳能、海水温度)
  * 我们想把它们细分为20个组，我们需要一个有`20^10`个分组的直方图——超过10万亿
  * **但只有10000个测量值，所以我们预计实际上每个分组都是空的，而且一小部分分组(在这个例子中是十亿分之一)可能每个分组都只有一个测量值。更不用说一个幼稚的实现将需要10万亿计数的内存空间——即使使用8位无符号整数，这将是10 TB 的内存**
* 这就是高维稀疏性问题 problem of sparseness in high-dimensions
  * 维数灾难: 随着维度的增加，概括变得越来越困难 **Curse of dimensionality: as dimension increases generalisation gets harder exponentially**

### box in high-d vector spaces

想象一个高维(超立方体)的空盒子

* 用随机点填充它
* 对于任何给定的点，在足够高的维度中，盒子的边界将比盒子中的任何其他点离他们更近。
* 不仅如此，每一个点与其他点的距离也几乎相同(欧几里得，L2)
  * 例如，一个20D 的盒子有超过100万个角
  * **维度越高，所有点之间的距离都近似相等**
  * ![](/static/2020-10-30-02-43-21.png)
* 对于 d > 5，更多的体积（点的体积？）在靠近角落的区域，而不是其他任何地方; 到 d = 20，压倒性的空间体积在角落
* **高维空间，（点-球）绝大多数点集中在球的边界附近 --- 所以可以应用于其他形状的空间->立方体内的球，球的体积在离原点更远的地方集中（在立方体表面边界）**
  * [参考](http://www.360doc.com/content/20/0517/01/1339386_912800159.shtml)
* **维度n趋近∞，n维球体积趋于0**

想象一下，一个球体放在盒子里，这样球体的表面刚好碰到盒子的边缘(一个内切球)。随着 d 的增加，球体占用的空间越来越小，直到它在空间中所占的比例消失殆尽。

用随机点填充内球面。它们几乎都在球体表面附近的一个小壳体内，中间几乎没有

## Matrices & Linear Operators

不会用于表示data，只用于表示矩阵操作（对向量的其他functions）

🍊 uses of matrices

![](/static/2020-10-30-03-06-06.png)

* 1d array表示real vectors实数向量（1d vector space）
* **1.矩阵Matrices用于表示2D实数数组的线性映射** represent linea map as 2D arrays of reals
  * `R^(m*n)`
  * **vector - points in space**
  * **矩阵表示对空间中点（vector）进行的操作**
* 2.矩阵表示的运算是**向量上应用的一类特殊函数**  particular class of （linear）functions on vectors
  * <font color="red">“刚性”变换rigid transformations</font>只有物体的位置(平移变换，向量？)和朝向(旋转变换，矩阵？)发生改变,而形状不变
  * 矩阵是记录这些运算的一种非常简洁的方法compact way

### Operations with Matrices

![](/static/2020-10-30-03-06-27.png)

* **矩阵加减**
  * `C = A+B`
* **缩放，缩放因子：标量**
  * `C = sA`
* **倒置**
  * `B= A^T`
  * 交换rows & columns
* **矩阵应用于向量(加权和，是矩阵*向量)**
  * applies a matrix to a vector
  * `y = Ax`
  * `n*m, m -> n`
* **矩阵相乘（外积？形成矩阵）**
  * composes the effect of 2 matrices
  * `C=AB`
  * `p*q, q*r -> p*r`

### matrix notation

矩阵表示，符号

![](/static/2020-10-30-03-21-33.png)
![](/static/2020-10-30-03-22-42.png)

* 手写通常，大写字母表示matrix
  * **通常不在代码中使用大写字母编写矩阵**——它们像其他值一样遵循变量命名的常规规则
* 维数为 `n × m` 的矩阵
  * n 行
  * m 列
  * **矩阵 A 的元素`ai,j`表示在第 i 行和第 j 列**
* <font color="red">注意区分与2D数组&矩阵</font>
  * 矩阵只是**线性代数意义上**用2D数组表示

### Matrices: as Linear Maps

* **1.矩阵Matrices用于表示2D实数数组的线性映射** represent linea map as 2D arrays of reals
  * `R^(m*n)`
  * **vector - points in space**
  * **矩阵表示对空间中点（vector）进行的操作**

🍊 矩阵表示（向量的）线性映射**linear maps** -- 应用于向量的线性函数这一操作，由矩阵表示

* 即，**应用于向量的**，输出向量的函数
* 在标准表示法中，**表示：应用于向量的矩阵**
  * `Ax = f(x)`using matrix to multiply vectors(means apply)
* <font color="red">这相当于将一些(线性)函数 `f (x)`应用到向量上。矩阵以非常紧凑的形式表示向量到向量的映射函数</font> matrices represent functions mapping vectors to vectors in compact form
  * 矩阵捕获了一组特殊的函数，这些函数保留了向量所作用的重要性质

#### Effect of Matrix Transform & Linearity

矩阵变换后结果

![](/static/2020-10-30-16-39-43.png)

* `nxm`矩阵`A`
  * 表示`f(x)`函数, 将`m`维向量转换为`n`维向量
    * 仍在一个线性空间？--线性变换
  * origin不会移动
* **变换：缩放，旋转的结合**

矩阵线性映射/函数（操作）的线性（性质）

![](/static/2020-10-30-16-47-08.png)

* `𝑓(𝐱+𝐲)=𝑓(𝐱)+𝑓(𝐲)=𝐴(𝐱+𝐲)=𝐴𝐱+𝐴𝐲`
* `𝑓(𝑐𝐱)=𝑐𝑓(𝐱)=𝐴(𝑐𝐱)=𝑐𝐴𝐱`

## 线性变换 vs 线性映射

![](/static/2020-11-02-15-24-53.png)

> 线性映射，从一个向量空间V到另一个向量空间W的映射。并保持加法&数量乘法运算。
>
> 线性变换，线性空间V到其自身的线性映射

🍬 线性的含义
* 原点保持固定
* 直线经过转换后，仍为直线（不弯曲）

线性变换

* 线性空间`V`到其自身的映射，**称为`V`上的一个变换**
  * **即，运算前后向量存在同一向量空间`V`中**
* 满足
  * `𝑓(𝐱+𝐲)=𝑓(𝐱)+𝑓(𝐲)=𝐴(𝐱+𝐲)=𝐴𝐱+𝐴𝐲`
  * `𝑓(𝑐𝐱)=𝑐𝑓(𝐱)=𝐴(𝑐𝐱)=𝑐𝐴𝐱`

线性映射（也叫做线性变换，线性fucntion）

* 在**两个向量空间之间的函数**。保持**向量加法vector addtion&标量乘法scalar multiplication**

## Geometric Intuition(Cube->Parallelepiped)

几何直观（立方体->平行六面体）

![](/static/2020-10-30-19-07-01.png)

* 理解矩阵运算的一种直观方法是考虑一个矩阵
  * **将一个向量空间中以原点为中心的立方体转换为另一个向量空间中的平行六面体**
  * **原点保持不变**
* 这是矩阵所能应用的唯一一种变换

### Transforms & Projections

* **linear map/function 线性映射/函数**
  * `f = R^m->R^n`，得到`nxm`矩阵
  * 满足线性性值
* **linear transform 线性变换**:如果由matrix表示的map代表`nxn`
  * 则表示从**一个向量空间到相同向量空间**的映射`R^n->R^n`
* **linear projection 线性投影**
  * 映射属性`Ax=AAx`或`f(x)=f(f(x))`
  * 如将3Dpoints投影至一个plane
  * <font color="red">对一组向量应用此变换(投影)两次，等于应用它一次</font>

🍬 if there is a function  𝑓(𝐱)  that satisfies the linearity conditions above, it can be expressed as a matrix  𝐴 如果有一个函数 f (x)满足上述的线性条件，它可以表示为矩阵 a

## Matrix Operations: 矩阵加法,乘标量

矩阵操作

![](/static/2020-10-30-20-05-10.png)
![](/static/2020-10-30-20-26-17.png)

* **linear algebra 线性代数**
  * 可以应用于矩阵
  * **矩阵加法 & 矩阵*标量**

## Application to vectos：矩阵*向量

将矩阵应用到向量上（形成矩阵和向量的加权和，`A@x`）

![](/static/2020-10-30-20-37-19.png)

* `Ax`
  * **matrix `A` applied to vector `x`**，相当于apply (linear)fucntion `f(x)`(由矩阵表示线性function)
* 如`A`矩阵，`R^nxm`。`x`向量`R^m`
  * **从`m`维向量空间映射到`n`维向量空间**
  * **矩阵的列必须对应向量的维数**
    * 如`3x2`矩阵和`3`维vector就不能应用`@`
    * 如`2x3`矩阵和`3`维vector最后得到`2`维向量

🍊 矩阵应用于向量 --- **形成向量中所有元素的加权和 form a weighted sum of the elements of the vector**

* 即，**linear combination 线性结合 = 向量元素加权和**
* <font color="red">对一个向量进行旋转&缩放的综合过程（线性变换过程）， 通过矩阵*向量，一个向量变换为另一个向量</font>
  * mxn的矩阵A，就是n维向量->m维向量上的变换

🍊 例子

![](/static/2020-10-30-20-38-52.png)
![](/static/2020-10-30-20-46-38.png)

* 将向量`x`中每个元素`x1,x2,...,xm`乘上`A`矩阵中相应字段。再将他门相加

```python
[1 2
3 4
5 6
7 8
]# matrix

[1 2]# vector

Ax
= 1 *[1,3,5,7] + 2 *[2,4,6,8]
= [5,11,17,23]

A @ x#表示 【矩阵A应用于向量x --形成向量中所有元素的加权和】，x每个元素xi*对应的A的每列字段Ai 最后相加
```

### 意义

![](/static/2020-11-02-15-20-39.png)

### Matrix Multiplication:满足条件

矩阵乘法（矩阵相乘）

![](/static/2020-11-01-14-49-56.png)

* *C=AB*
* 如果`A`代表线性变换`f(x)`,`B`为线性变换`g(x)`
  * 满足矩阵乘法`BAx=g(f(x))`
* <font color="red">矩阵相乘相当于合并两个线性函数 composing the linear function</font>
* <font color="red"> composition of linear maps 从右向左分析</font>
  * `BA`, apply transformation `A`, then `B`, form the product `BA`

### Multiplication：矩阵*矩阵

乘法算法

![](/static/2020-11-01-14-50-25.png)
![](/static/2020-11-01-14-49-56.png)
![](/static/2020-11-01-14-52-02.png)
![](/static/2020-11-01-15-00-09.png)
![](/static/2020-11-01-15-55-13.png)

* 缩放matrix & 旋转matrix的乘法积
  * scale and rotate matrix 缩放旋转矩阵
* **乘法，矩阵满足要求**
  * `A`, `pxq`矩阵，表示`R^q->R^p`的映射
  * `B`, `qxr`矩阵，表示`R^r->R^q`的映射
* A匹配B的维度（**A的列等于B的行**），否则乘法undefined
* **`np.dot(a,b)`或`a @ b`**
  * 矩阵乘法

### Time complexity of multiplication

![](/static/2020-11-01-15-56-40.png)

## Apply Matrices to Vectors：矩阵*向量

矩阵应用于向量

![](/static/2020-11-01-20-08-22.png)

* `m`维向量
  * 由`mx1`列向量表示(只是为了方便理解)【横->竖】
* 矩阵`A`
  * 满足`nxm`（A是个线性映射）
  * 如满足`mxm`（A是个线性变换）
* `Ax`
* `A @ x`

## Transposition

矩阵倒置

![](/static/2020-11-02-14-35-55.png)

* `A^T`
  * rows & columns交换
* `A.T`

<font color="red">计算协方差矩阵时可能需要用到，注意维度和样本到底是哪个</font>

* `np.cov()`

## Column & Row Vectors：内积/外积

`mx1`列向量column vector

* 的倒置，是**行向量row vector**

🍊 行向量 & 列向量的积（matrix multiplication，外积）

* `Mx1` - column vector
* `1xN` - row vector
* 矩阵相乘`MxN` matrix(外积为矩阵，A的行×B的列)

### 内积/外积

🍊 outer/inner product

![](/static/2020-11-02-14-58-00.png)
![](/static/2020-11-02-14-58-13.png)

* **outer**
  * `Mx1` & `1xN` -> **MxN的矩阵**
  * `np.outer(x,y)`
  * `x.T @ y`
  * <font color="red">满足，A的列等于B的行，才能进行矩阵相乘</font>
* **inner**
  * `1xN` & `Nx1`的向量-> **`1x1`（矩阵）标量**
  * <font color="red">x,y向量/矩阵必须是同型/长度相同矩阵</font>
  * `np.inner(x,y)`
  * `x @ y.T`

## Composed Maps: 注意Order

重要性质

* `A` - `f(x)`
* `B` - `g(x)`
* 则矩阵相乘`BA`-`g(f(x))`
  * 外积 - compostion
* <font color="red">注意映射的顺序</font>
  * `BAx=B(Ax)`
  * 先`A`应用于`x`（进行映射？x向量被变换，缩放&旋转）`mxn`&`n`->`m`行向量
  * 在用`B`乘，transfer to other vector space

🍊 例子

![](/static/2020-11-02-15-51-14.png)
![](/static/2020-11-02-15-51-29.png)

```python
# rotation: 30 degree
d30 = np.radians(30)
cs = np.cos(d30)
ss = np.sin(d30)
rot30 = np.array([[cs, ss], [-ss, cs]])

# scaling
scale_x = np.array([[1,0],[0,0.5]])
```

![](/static/2020-11-02-15-53-12.png)
![](/static/2020-11-02-15-53-27.png)

```python
# multipy get the product
# Rotate then scale: be careful with the order
A = scale_x @ rot30

B = rot30 @ scale_x

# A &B not the same
```

### Commutativity

通常`AB!=BA`

![](/static/2020-11-02-15-57-44.png)

* 因为乘法仅对`pxq`&`qxr`的矩阵定义
  * 除非`p=q=r`，不然**就算交换顺序**（`qxr`乘`pxq`），**两矩阵也不能相乘**
  * 如果`p=q=r`，两矩阵为方阵，**也需要考虑operands的顺序**

### 略过

![](/static/2020-11-02-16-01-52.png)
![](/static/2020-11-02-16-01-59.png)

## 协方差矩阵measuring spread: Covariance Matrices

协方差矩阵

![](/static/2020-11-02-16-09-42.png)
![](/static/2020-11-02-16-10-12.png)

* 用于measure the spread of a dataset
  * **variance方差**: sum of **squared differences** of **each element from the mean** of the vector ?
  * 研究随机变量与其均值的偏离程度
* **标准差`σ` standard deviation**
  * 方差variance的平方根
  * measure how "spread out" a vector of values `x` is

🍊 协方差的计算

![](/static/2020-11-02-16-27-12.png)

* A样本（矩阵）的每个元素-A的均值 * B样本每个元素-B均值。
  * 最后除样本元素数
* **协方差结果**
  * >0 正相关
  * =0 没关系，一样
  * <0 负相关
* **相关的程度：相关系数**
  * 两个维度协方差/两个维度标准差

### 协方差矩阵

协方差

* 针对**一维样本**集合，
  * 协方差就是方差
  * **协方差意义：和方差一样，反应集合中各元素离散程度**
* 针对二维样本集合，
  * **协方差意义：反应两个维度之间的相关程度（两个维度之间数据是否有关系）**
  * 正相关
  * 负相关
  * 无关
* 针对三维样本集合，
  * 各个维度总体相关性，各个维度之间的关系（样本A,B,C...）

🍬 二维以上计算协方差

![](/static/2020-11-02-16-38-48.png)
![](/static/2020-11-02-16-40-39.png)
![](/static/2020-11-02-16-48-14.png)
![](/static/2020-11-02-16-50-49.png)

* 用**协方差矩阵**
  * <font color="red">show the spread of each dimensions</font>
* 3列字段，3个维度，
  * 看各个维度之间是否有关系
* 最后**协方差矩阵为方阵squared&对称symmetric&半正定positive semi-definite矩阵**
  * `维度x维度`
  * <font color="red">斜对角线上的元素 - 该维度方差，其他元素为维度之间协方差</font>
* average squared difference of **each column of data** from **the average of each column**
* `np.cov(x)`

![](/static/2020-11-02-16-55-37.png)

```python
x = np.random.normal(0,1,(500,5))
mu = np.mean(x, axis=0)
sigma_cross ((x-mu).T @ (x-mu)) / (x.shape[0]-1)

sigma_np = np.cov(x, rowvar=False)#same result
```

## Covariance Ellipses

![](/static/2020-11-02-16-57-53.png)

* mean vector
  * **center of the ellipse**
* covariance matrix
  * **the shape of ellipse**
* <font color="red">error ellipse</font>
  * very useful for summary of high-dimensional data

## Anatomy of Matrix

diagonal

* `Aii`
* Matrix elements

diagonal matrix对角矩阵

![](/static/2020-11-02-17-06-45.png)

* **all zero** except for a single diagonal entry
  * 里面非`0`的元素 - diagonal
  * 为`0`的元素 - off diagonal
* 它们表示一个转换**pure scaling no rotation**，是每个维度的独立缩放。这样的矩阵将立方体转换为长方体，(即所有角度保持不变，不发生旋转)

```python
np.diag([1,2,3,3,2,1])#转换向量为对角矩阵

```

anti-diagonal

![](/static/2020-11-02-17-11-24.png)

```python
np.fliplr(np.diag([1,2,3]))
```

## Special forms of Matrix

### 单位矩阵 identity

![](/static/2020-11-02-17-12-53.png)
![](/static/2020-11-02-17-14-32.png)

![](/static/2020-11-02-17-16-03.png)
![](/static/2020-11-02-17-16-16.png)
![](/static/2020-11-02-17-15-10.png)

* diagonal - all`1`
* does nothing
  * `IA=A=AI`
  * `Ix=x`
* `np.eye(3)`
  * 生成3x3的单位矩阵

### Zero

![](/static/2020-11-02-17-16-56.png)

```python
z = np.zeros(4,4)
z @ x
z @ y
```

### Square

![](/static/2020-11-02-17-18-07.png)

* map from `R^n` -> `R^n`

### Triangular:三角矩阵

![](/static/2020-11-02-17-19-37.png)
![](/static/2020-11-02-17-19-48.png)
![](/static/2020-11-02-17-19-55.png)

* **upper triangular**
  * only have elements above the diagonal 对角线及以上有元素
* **lower triangular**
  * only have elements above the diagonal 对角线及以下有元素

可以立用来建立方程组

```python
upper = np.tri(4)
lower = np.tri(4).T
```

## np.linespace：生成间隔

作用为：在指定的大间隔内，返回固定间隔的数据。他将返回“num”个等间距的样本，在区间[start, stop]中。其中，区间的结束端点可以被排除在外

![](/static/2020-11-08-01-11-15.png)

## 向量：内积，外积

内积

![](/static/2020-11-08-16-30-51.png)

外积

![](/static/2020-11-08-16-31-28.png)