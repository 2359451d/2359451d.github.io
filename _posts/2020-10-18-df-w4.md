---
layout: post
title: "DF Week4 - Computational Linear Algebra I"
date: 2020-10-18
excerpt: "data fundamentals笔记"
tags: [学习笔记, data fundamentals, 2020, python]
feature: https://miro.medium.com/max/1400/1*gHamsYEMBoVIWbTbn3CQ-A.gif
comments: true
---

[reference: week_4_Computational Linear Algebra I.html
](https://moodle.gla.ac.uk/pluginfile.php/1241379/mod_folder/content/0/week_4_matrices_i.pdf?forcedownload=1)

infomation is filtered，，已筛选部分用得上的

* 目录
{:toc}

## Outline

you should know

* **vector & vector space**
  * standard operations on vectors: add & multiplication
* **norm 范式**
  * how it can be used to measure vectors 如何使用norm测量vector
* **inner product 向量内积**
  * 向量内积如何产生向量的几何形
* **mathematical vectors** map onto **numerical arrays** 数学矢量如何映射到数字数组上
* 不同的**p规范(p-norm**)及其用法
* 向量表示的重要计算用途
* how to **characterise vector data with a mean vector and a covariance matrix** 如何用平均矢量和协方差矩阵表示矢量数据
* properties of high-dimensional vector spaces **高维向量空间的性质**
* basic notation for matrices 矩阵的基本符号
* view of matrices as linear maps 矩阵视图 - 线性映射
* how basic geometric transforms are implemented using matrices 使用**矩阵实现基本的几何变换**
* how matrix multiplication is defined and its algebraic properties 定义**矩阵乘法及其代数性质**
* the basic anatomy of matrices **矩阵的基本组成**

## Example: Text

string-弱结构，无法贡献于机器翻译系统

* 可以给文本片段**添加额外数学结构**
  * **将文本片段放置在向量空间(vector space)中**
  * **片段可以是单词、部分词或整个句子。这就是所谓的嵌入**算法Word2vec可以学习从字符串到高维向量(通常大于100D)的转换，只需观察大量的文本
* **虽然向量空间每个维度没有特定意义，但是嵌入embedding意味着语义semantics映射到空间关系上**

## Vector spaces

本课考虑向量是实数有序元组

* `[ x1，x2，... xn ] ，xi ∈ r`
* 向量固定维度`n`
  * **元组长度 length of the tuple**
* 可以假设向量中**每个元素**都为**与其他元素正交方向上的距离 a distance in an direction orthogonal**

🍊 例子 - 有需要记的概念

* **长度为3的向量（3维向量）**，可以用于表示笛卡尔坐标系中的空间位置
  * 每个向量有3个正交的测量量 3 **orthogonal measurements** for each vector
  * orthogonal - independent **正交即独立，即，=90°**
* 3D vector`[5,7,3]`
  * `ℝ^3`
  * 组成 `5*[1,0,0] + 7*[0,1,0] + 3*[0,0,1]`
  * 可看为**正交单位向量的加权和 weighted sum of orthogonal unit vectors**
  * 该向量空间有3个独立basis vectors, 因此为三维（向量长度也为3）
* `[1,0,0], [0,1,0], [0,0,1]`
  * **unit vector (basis vectors)**
  * 每个向量，长度为`1`
  * 都指向一个单独的方向(正交方向) orthogonal direction

🍊 向量表示

![](/static/2020-10-28-01-02-12.png)

* 用**小写加粗字母**表示向量

## Points in Space

### Notation

标记`ℝ^𝑛`

![](/static/2020-10-28-01-05-43.png)

* `ℝ`实数集
* `ℝ≥0`非负实数集合
* `ℝ^n`N 个实数的元组的集合 the set of tuples of exactly `n` real numbers
* `ℝ^n*m` n行m个元素的2维实数矩阵集合
* `(ℝ^𝑛,ℝ^𝑛)→ℝ` map：一对n维向量-实数的映射对
  * a map from a pair of n-dimensional vectors to a real number

## Vector spaces

![](/static/2020-10-28-01-16-47.png)

* 任何**规定了维数`n`的向量都位于【向量空间 `ℝ^𝑛`】**
  * 目前向量空间只考虑有限维度的实数向量空间（标准单位基本向量）【还有复数，无限维度空间】
  * **该向量属于，任何长度为`n`的，元素为实数的向量集合中**

🍊 且在该向量空间`ℝ^𝑛`下，规定了如下操作

* **标量乘法/标量积 `ax`** scalar multiplication
  * 对于任何标量`a`，都定义了`ax`（标量积？）
  * 对于实数向量，`ax=[ax1,ax2,...,axn]`元素缩放 elementwise scaling
  * ![](/static/2020-10-28-01-29-19.png)
* **向量加法 `x+y`** vector addition
  * `x+y`个向量， `x`& `y`维数相同
  * **实数向量**，`𝐱+𝐲=[𝑥1+𝑦1,𝑥2+𝑦2,…𝑥𝑑+𝑦𝑑]`元素和
  * ![](/static/2020-10-28-01-32-07.png)
* **norm `||𝐱||` 范式**
  * 测量向量长度
  * `ℝ𝑛→ℝ≥0`
  * ![](/static/2020-10-28-02-32-52.png)
* **向量内积 `⟨𝐱|𝐲⟩  or 𝐱∙𝐲`** inner product
  * **可以比较2个向量夹角 angle**
  * <font color="red">2个正交向量的向量内积为`0`，angle=90</font>
  * **实数向量的向量内积，即dot product点积** `𝐱∙𝐲=𝑥1𝑦1+𝑥2𝑦2+𝑥3𝑦3…𝑥𝑑𝑦𝑑`
  * ![](/static/2020-10-28-02-33-09.png)

🍬 向量间所有操作都在同一向量空间中定义

* 不同维度的向量，不属于同一向量空间，不能进行如上定义的操作

### topological & inner product spaces

拓扑&内积空间

![](/static/2020-10-28-02-41-33.png)

* 有了范式norm， 向量空间就是**拓扑向量空间**，意味着
  * 向量空间是连续的，且向量“紧靠在一起”（或向量周围有领域neighbourhood）
  * 对于向量内积，**向量空间即内积空间（内积集合？）inner product space**

🍊 空间中的向量点vector points、箭头指向，是从原点还是数字元组？

![](/static/2020-10-28-02-47-06.png)

* 数字元组？**或想象这些向量点在向量空间中**
* 向量表示数据，数据在向量空间中
  * vectors to represent data; data lies in space
* 矩阵由对数据的操作得来，矩阵扭曲向量空间
  * matrices to represent operations on data; matrices warp space

### Relation to arrays

与矩阵关系

* 实数向量可以用一维浮点数组来表示，在本系列的第一讲中，我们称之为“向量”
* 但是要小心，**表示和数学元素是不同的东西**，就像浮点数不是实数一样

![](/static/2020-10-28-03-04-57.png)
![](/static/2020-10-28-03-05-55.png)

标量积
![](/static/2020-10-28-03-06-21.png)
![](/static/2020-10-28-03-06-27.png)

![](/static/2020-10-28-03-08-18.png)
![](/static/2020-10-28-03-08-52.png)

* 向量相加
  * `x+y`
* 范式
  * `np.linalg.norm(x)`
* 点积，向量内积
  * `np.dot(x,y)`

### uses of vectors

向量使用

* **composed**
  * addition
* **compared**
  * norms/inner products
* **weighted**
  * scaling

🍬 向量可以对数据进行各种变换

* 因此vector->ndarray映射，可以更高效简洁操作

### Vector Data

数据集dataset，通常以**2D表格**存储

![](/static/2020-10-28-20-11-28.png)

* 可以看为向量列表 lists of vectors
* 每行 - 代表“observation”的vector
  * 每条观察记录stacked up组成2D矩阵
* 每列 - 跨多条观察值的一种元素

🍊

* 以上例子，每行可看为一个在向量空间`ℝ^4`中的向量
* 以上整个矩阵是同一个向量空间中的**向量序列**。这意味着我们可以对表格数据进行**几何描述 geometric**

### Geometric Operations

矢量最明显用途

* 表示二维/三维几何数据
* 游戏or3D渲染引擎中所有计算都由大规模重复**低维向量操作（2D,3D,4D）**组成

![](/static/2020-10-28-20-35-25.png)

图形化管道将所有东西(空间位置、表面法线方向、纹理坐标、颜色等)都处理为大量的矢量数组Graphical pipelines process everything (spatial position, surface normal direction, texture coordinates, colours, and so on) as large arrays of vectors

* 在 GPU 上进行图形编程主要涉及将**数据打包到低维向量数组**(在 CPU 上) ，然后使用**着色器语言在 GPU 上快速处理它们** Programming for graphics on GPUs largely involves packing data into a low-dimensional vector arrays (on the CPU) then processing them quickly on the GPU using a **shader language**

### Machine Learning Applications

机器学习很大程度上依赖向量表示

![](/static/2020-10-28-21-24-33.png)

* 将一些数据转换成**特征向量feature vectors**
  * 特征向量：向量空间中数据编码（如表格数据）
  * describe some data point that might be a vector representing a word image
* 创建一个函数，将**特征向量转换**为预测prediction(例如类标签)
  * 特征转换：处理raw data, 输出特征向量

🍊 最简单有效的机器学习算法之一

> k近邻算法：给定一个训练集，新输入实例。**在训练集中找到与该实例最邻近的k个实例**。
>
> 这**k个实例多数属于某个类**，就把该新输入实例分类到这个类中

* **k nearest neighbours**k近邻算法
  * 涉及data训练集，包含`xi,yi`对
    * 特征向量`xi`，标签`yi`
* 当一个新特征需要分类进行预测时，**计算该训练集中的 k 个最近的向量，使用范数计算距离**
  * 输出预测（把新输入归到多数类中）是这些 k 近邻实例中**出现次数最多的类标签**(k 在某种程度上是预设值; 对于许多问题，它可能在3-12左右)

### Image Compression

![](/static/2020-10-28-22-52-07.png)
![](/static/2020-10-28-22-54-56.png)

* 8x8 image patch could be unraveled to a 64-d vector
  * these vectors could be treated as elements of a vetor space

## Basic Vector Operations

### Add & Multiply

Elementwise addition & scalar multiplication 对位相加 & 标量积 ---【weighted sum】

![](/static/2020-10-29-13-36-50.png)
![](/static/2020-10-29-14-44-27.png)

* 形成**weighted sum** of vectors
* `𝜆1𝐱1+𝜆2𝐱2+⋯+𝜆𝑛𝐱𝐧`
* only applies to vectors of the same dimension

🍊 因为defined addtion & scalar multiplication

![](/static/2020-10-29-14-38-26.png)
(10 subdivisons of the line, find intermediate points accrording to linear interpolating---因此通过线性插值，可以找到**两向量间的中间点向量**，等)

* 可以应用标准统计量&操作
* 如，两向量的**linearly interpolate 线性插值**
  * 两值的线性插值由**参数`α`**控制
  * `lerp(𝐱,𝐲,𝛼)=(1−𝛼)𝐱+(𝛼)𝐲`
    * `(1-α)`scaling factor
  * **线性插值：沿着两个向量之间的直线移动: 当 α 从0到1时，得到结果是从 x 到 y 的一条平滑的直线** --- find a new vector between the line x~y

### 向量欧几里得长度: How big is the vector

可以考虑给向量空间**定义距离 distance**

🍊 向量的**欧几里得长度/范数（向量大小）**（Euclidean length/norm of vectors）

![](/static/2020-10-29-15-11-08.png)
![](/static/2020-10-29-15-06-53.png)

* `||𝐱||`
  * 向量长度表示向量大小？--单个向量的范数=其各维平方和,的平方根
* `np.linalg.norm()`

🍊 球体内向量距离 -半径？

![](/static/2020-10-29-15-11-18.png)

### 两向量距离&lp/minkowski范数：Different Norms

默认范数

![](/static/2020-10-29-15-23-07.png)

* **欧几里得范数/标准 Euclidean norm/distance measure**
* **包含有欧几里得范数的实数向量的向量空间称为---欧几里得空间**
* **两向量间的距离 --- 两向量间差的范数** distance between 2 vectors - norm of the difference of 2 vectors 
  * `||𝐱−𝐲||2`

🍊 也有其他方法**测量一个单向量的范数/长度** (都是欧几里得范数的延申)

![](/static/2020-10-29-15-21-49.png)
![](/static/2020-10-29-15-44-43.png)
![](/static/2020-10-29-15-50-16.png)
![](/static/2020-10-29-21-04-36.png)

* <font color="red">某些特定上下文中，适用</font>
* **Lp-norms**
  * `‖𝐱‖𝑝`
  * 是一组范数
  * `xi^p` - element, to the power `p`
  * **向量`x`中每个元素`xi`取`p`次方，求和。最后开`p`次方根**
  * <font color="red">控制模型复杂度减少过拟合。一般在损失函数中加入惩罚项</font>
* **`p`具体看是L1还是L2(欧几里得)还是其他范数**
  * **L0范数 - 表示向量x中非0元素的个数**![](/static/2020-10-29-23-00-16.png)
  * **L1范数 - 等于向量中所有元素绝对值之和**![](/static/2020-10-29-23-01-14.png)
  * **L2范数 - 范数表示向量（或矩阵）的元素平方和（再开根）**![](/static/2020-10-29-23-01-46.png)
* L∞可以近似看为`(xj^∞)^1/∞`，maximum
  * +∞ maximum
  * -∞ minimum
* **Minkowski norms 闽可斯基距离**

🍊 不同范数&上下文应用

![](/static/2020-10-29-15-50-48.png)

### Unit Vectors & normalisation

单位向量

![](/static/2020-10-29-22-49-56.png)(length 1 from the origin)
![](/static/2020-10-29-22-50-07.png)

* 范数为`1`
  * **取决于选用的范数**
* unit vector 总指向欧几里得范数为`1`的向量

归一化/标准化（欧几里得范数）

* **将向量`x`缩放`1/||x2||`倍?**
  * `||x2||` - **L2 norm, Euclidean norm**
* 向量坐标都除于向量的长度
  * 新向量的长度恰好为1

```python
x = np.random.normal(0,5,(5,))#random x vector
x_norm = x/np.linalg.norm(x) # x norm after nomalisation( length of 1)

```

### Inner Products

向量内积（点积）

![](/static/2020-10-29-23-37-39.png)

* 结果为标量IR
* 可求两向量间夹角
* `x·y = ||x||||y||cosθ`
* `x·y = x1*y1 + x2*y2 +...+xiyi`
  * 每维元素对位相乘的连加和
* <font color="red">非单位向量的点积，可以先标准化？</font>
* <font color="red">两个平行单位向量，点积为1</font>
  * cosθ=0=x*y

```python
np.inner(x,y) # inner product of x and y
```

🍊 内积可用于比较不同大小的向量

* 因为只依赖于向量方向
* 如，它被广泛应用于信息检索
  * 比较文档向量document vectors，这些文档向量将文档中的术语表示为大or稀疏的向量
  * 对于长度不同的文档，这些向量的大小可能存在很大差异

## Basic Vector statistics

向量可有其他操作（统计量相关）

![](/static/2020-10-29-23-56-19.png)

* 只需应用加法，标量积，外积等定义

### Mean vector

🍊 平均向量

* 一组`N`个向量的mean vector是这些向量的和乘`1/N`

