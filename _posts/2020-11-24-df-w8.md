---
layout: post
title: "DF Week8 - Probability I"
date: 2020-11-24
excerpt: "data fundamentals笔记"
tags: [学习笔记, data fundamentals, 2020, python]
feature: https://miro.medium.com/max/1400/1*gHamsYEMBoVIWbTbn3CQ-A.gif
comments: true
---

infomation is filtered，，已筛选部分用得上的

* 目录
{:toc}

## Outline

* 概率是什么
  * 不同的哲学解释
* 逆向/正向概率
* 随机变量，分布，概率质量/密度函数
* 经验分布是什么
  * 如何从数据中计算出来
* 期望 & 期望值
* 概率论公理
* 条件，边缘，联合分布
* 熵
  * 如何计算
* 贝叶斯规则 & 应用问题
* bigrams
  * 定义&如何使用
* 概率表示为
  * 赔率 odds
  * 对数赔率 log-odds
  * 对数概率 log-probability
  * 以及这些数值的效果

## 概率定义：Probability

本节课程涉及随机元素; 不确定性、随机性和统计学在计算中的作用。基本的数学原理来自概率论，它给我们提供了操纵不确定值的简单而有力的方法，并且让我们做有用的运算，比如根据一些观察推断最可能的假设。概率论是操纵不确定性的一种简单、一致和有效的方法

---

### 概率定义：What is Probability

![](/static/2020-11-24-15-11-50.png)

* **Experiment/ trial - 实验/试验**
  * 结果不确定的事件
  * 如，潜艇失联，潜艇位置未知
* **Outcome - 结果**
  * 实验的结果； 世界的一种特殊状态
  * 如，潜艇位于海洋网格的方格中
* **Sample Space - 样本空间**
  * 实验中所有可能**结果的集合**
  * 例如，海洋网格，[0,0],[0,1]...[9,9]
* **Event - 事件**
  * **具有共同属性**的可能**结果的子集**
  * 例如，位于赤道以南的方格
* **Probability - 概率**
  * 相对于样本空间的概率，概率事件是**事件中（样本空间中的）结果数/样本空间中的结果总数**
  * 是个比率，因此总是介于`0（不可能的事件）`&`1（必然事件）`之间的一个实数
  * 如，潜艇位于赤道以下的概率，潜艇位于[0,0]的概率。这种情况下，事件只是单一结果
* **Probability Distribution - 概率分布**
  * 结果与概率之和为`1`的映射
  * 因为概率`1`时，实验结果必须发生，因此**所有可能结果之和加起来为`1`**.
  * 随机变量有概率分布，将每个结果映射为概率
  * 例如`P(X=x)`, 潜艇位于网格x中的概率
* **Random Variable - 随机变量**
  * 代表一个未知变量，我们知道它的概率分布。该变量与实验结果相关。
  * 例如`X`代表潜艇位置的随机变量
* **Probability density/mass function - 概率密度/质量函数**
  * 定义概率分布， 将每个结果映射到一个概率 `fx（x），x->R`
  * 可以实`x（density）`上的**连续函数**，也可是`x(mass)`上的离散函数
  * 例如，`fx(x)`可为潜艇的概率质量函数，将每个**网格**映射到**实数（代表其概率，每个网格的概率）**
* **Observation - 观察**
  * 直接观察到的结果，即， data
  * 例如，潜艇被发现在[0,5]的网格中
* **Sample-样本**
  * 已经模拟了一个概率分布的结果。从一个分布中，抽取了一个样本
  * 如，我们相信潜艇按照某种模式分布，那么可生成遵循这种模式的具体网格位置
* **Expectation/expected value - 期望值**
  * 随机变量的“平均值”
  * 如，潜艇位置的网格平均值为[3.46, 2.19]

---

In prose - 文字表示

![](/static/2020-11-24-16-29-14.png)

随机变量`X`有概率分布`P(X)`,对属于样本空间`x`的结果`x`,给出`0≤P(X=x)≤1`的概率

这个概率分布，由概率密度/质量函数`fx（x）`定义，赋予每个结果了概率，（满足每个结果的概率之和为`1`， `∑x∈xfX(x)=1`）

我们可以从分布中观测特定作为实验的结果`xi`

给定一个概率分布`P（X）`，可以对其新结果`x'j`进行抽样（模拟）

假设结果有值，可以通过无限次实验，计算出平均期望值`E[X]`

## 概率哲学：Philosophy of Probability

![](/static/2020-11-24-16-37-55.png)
关于概率及其使用，有两种思想流派。我们将(大部分)遵循贝叶斯的解释，但值得理解这需要什么。

### 贝叶斯/拉普拉斯观点： Bayesian/Laplacian view on probability

![](/static/2020-11-24-16-40-18.png)
贝叶斯人把概率看作是信念的微积分; 在这种思维模式中，概率是信念程度的度量。

`P (a) = 0`意味着事件 a 不可能为真，

`p (a) = 1`是事件 a 绝对确定

从贝叶斯的角度来看，说“外面下雨的概率是0.3”是有意义的(概率量化了我们对天气的信念，给出了我们所掌握的信息)。

请注意，这并不是说我们相信天气是0.3概率的雨天(不管这意味着什么)

![](/static/2020-11-24-16-42-06.png)

贝叶斯允许通过概率规则对状态的信念进行组合和操纵。**贝叶斯逻辑的关键过程是信念的更新**。给出一些

* **先前的信念**（这是格拉斯哥，不太可能是晴天）和一些
* **新的证据**（里面似乎有光明的反映），我们可以
* **更新我们的信念以计算后验**-我们在室外出现晴天的新概率

贝叶斯推理要求我们接受先于事件的先验，即必须用概率分布明确量化我们的假设。它是逻辑对不确定信息的一种扩展。

![](/static/2020-11-24-16-45-50.png)

### 频率论者的概率观: Frequentist View of Probability

![](/static/2020-11-24-16-58-54.png)

还有另一派认为概率只是**重复事件的长期行为**（例如，硬币在0.5内出现正面的概率，因为从长期来看，这将是这种情况发生的平均比例）

频率主义者不接受 "刚才是晴天的概率是多少？"这样的短语，因为不涉及长期行为（"现在"只出现一次）。

在这种世界观中，谈**论只能发生一次的事件的概率是没有意义的**。在频率主义者的观点中，问这样的事情是有意义的，比如 "在任何一天都会是晴天的概率是多少？"

* 因为我们可以测量这个事件（晴天或不晴天）的**许多不同的日子**
* 例如，频繁主义者不会给天蝎号在特定的格子方格中分配一个概率；这不是一个可以重复的实验

### 客观性和主观性：Objectivity & Subjectivity

![](/static/2020-11-24-17-01-39.png)

频繁论者与贝叶斯论者的辩论很快就进入了哲学领域。观点的多样性和争论的深度在这里无法做到公正。

很简单，贝叶斯概率论有时被说成是主观的，因为它需要先验信念的具体化，而频繁论概率模型不承认先验概念，因此是客观的。

另一种观点认为，贝叶斯模型明确地对不确定的知识进行编码，并陈述了操作该知识的普遍形式规则，就像形式逻辑对确定的知识一样。频繁论的方法是客观的，因为它们对普遍真理（如渐变行为）做出了陈述，但它们并没有形成信念的微积分，因此不能直接回答许多重要问题

---

![](/static/2020-11-24-17-04-28.png)

贝叶斯

* 包括**先验priors**
* 概率是一种信念的程度
* (被视为随机变量的人口参数，数据已知)

频率论者

* 无先验
* 概率是事件的长期频率
* (假设人口参数是固定的，数据是随机的)

## 概率模型的优越性：Superiority of probabilistic models

![](/static/2020-11-24-17-04-37.png)

不管你赞同什么样的哲学模型，有一件事你可以肯定: 概率是最好的。

除了概率论之外，有时还使用其他的不确定性模型。但是，所有其他的不确定性表示方法都严格地低于概率方法，即当存在不确定性时，使用概率模型对未来事件下 "赌注 "的人、代理人、计算机在所有决策系统中的回报率是最好的。

任何一种理论，如果赌注的结果和使用概率论的结果一样好，那么这种理论就相当于概率论

## 生成模型：正向概率和逆向概率：Generative models: forward and inverse probability

![](/static/2020-11-24-17-06-17.png)

概率模型的一个关键思想是生成过程，**即存在一些未知的过程，其结果可以观察到**

* 这个过程本身是由未观测到的变量控制的，我们不知道这些变量，但是我们可以推断出它们

![](/static/2020-11-24-17-07-21.png)
![](/static/2020-11-24-17-07-32.png)

经典的例子是一个骨灰盒问题。考虑一个骨灰盒，其中有许多球被倒入（由一些神秘的实体，说）。每个球可以是黑色或白色。

你从罐子里随便拿出四个球，观察它们的颜色，你会得到四个白色的球。

现在你可以问很多问题:

* 下一个被抽中的球是白球的概率是多少？
  * 这是一个**正向概率问题**。它问的是与**观测值的分布有关**的问题。
* 瓮中的白球和黑球的分布是怎样的？
  * 这是一个**反概率问题。它问的问题与未观察到的变量有关**，这些变量支配着产生观察结果的过程。
* 谁是神秘的实体？
  * 这是一个不可知的问题。我们所做的观察不能解决这个问题

有大量的过程可以被框定为骨灰盒问题（骨灰盒中的球被更换，有多个骨灰盒而你不知道球来自哪个骨灰盒的问题，球可以在骨灰盒之间移动的问题，等等）

## 概率论的形式基础 & 概率公理: A formal basis for probability theory & Axioms of probability

![](/static/2020-11-24-17-21-58.png)

概率只有几个基本公理，其他的都可以从这些公理中推导出来。

写出 `P(A)` 表示事件 A 的概率（注意：这些适用于事件（结果集），而不仅仅是结果！

* **有界性 - Boundedness**
  * `0≤P(A)≤1`
  * 所有可能事件`A`， 概率为`0`或正，小于`1`
* **统一性 - Unitarity**
  * `∑A P(A)=1`
  * 可能的**结果**的完整集合（不是事件！）
  * `A∈σ`在样本空间`σ`中--总有事情发生
* **Sum Rule**
  * `P(A∨B)=P(A)+P(B)−P(A∧B)`
  * 事件 A 或 B 發生的概率是獨立概率之和減去兩者發生的概率。
  * (注解：`∨`表示 "或"，`∧`表示 "和")
* **条件概率 - Conditional Probability**
  * ![](/static/2020-11-24-17-29-22.png)
  * 条件概率`P(A|B)`被定义为，在我们已经知道`B`**已经发生的前提下，事件`A`将发生的概率**

## 随机变量和分布: Random variables and distributions

![](/static/2020-11-24-17-31-06.png)

**随机变量**是一个变量，它可以有不同的值，但我们不知道它有什么值；

* 即一个"未分配"的变量
* 然而，我们有一些知识，**它捕捉了变量可能采取的状态，以及它们相应的概率**
* **概率论使我们能够在不给随机变量分配一个特定值的情况下对其进行操作**

🍊 随机变量用大写字母写，比如 `X`

一个随机变量可能代表:

* the outcome of dice throw (discrete); **掷骰子的结果(离散)** ;
* whether or not it is raining outside (discrete: binary); **外面是否下雨(离散的: 二进制**) ;
* the latitude of the USS Scorpion (continuous); **美国军舰蝎子号的纬度(连续**) ;
* the height of person we haven't met yet (continuous). **未曾谋面的人的高度（连续**）

🍊 在下一个单元中，我们将看到如何将随机变量实现为编程语言中的一类值，就像浮点数或整数一样

## 概率分布：Distributions

![](/static/2020-11-24-17-34-37.png)

概率分布定义了一个随机变量的不同状态的可能性。

我们可以把`X`看作是实验，`x`看作是结果，

* 用一个函数把**每一个可能的结果映射成一个概率**
* 我们写成`P(X=x)`(注意情况!)，并使用简写符号
  * `P(X=x)`，随机变量`X`取值`x（结果，一种状态）`的概率`P`
  * `P(X)`，`X=x`的概率的简写
  * `P(x)`，**具体数值**`X=x`的概率`P`的简写

🍊 我们可以把**一个结果（状态）看作是一个随机变量，取一个特定的值**，即`P(X=x)`

* 请注意，按照惯例，**我们用`P(A)`来表示事件`A`的概率，而不是随机变量A**（<font color="red">事件是一组结果，随机变量只对结果进行概率赋值）</font>

### Discrete and continuous random variable：离散/连续随机变量

![](/static/2020-11-24-17-41-37.png)

🍊 **随机变量**可以是**连续的**(例如一个人的身高)或**离散的**(骰子表面显示的数值)

* **离散变量**
  * 离散随机变量的**分布**用一个**概率质量函数(PMF)来描述，它给每个结果一个特定的值**；
  * 想象一个Python字典将结果映射到概率上。PMF 通常写成 `fX(x)` , 其中 `P(X=x)=fX(x)`
* **连续变量**
  * 一个连续变量有一个**概率密度函数(PDF)，它将概率在结果上的分布指定为一个连续函数 `fX(x)`**
  * 对于PDF来说，**并不是说** `P(X=x)=fX(x)`

### 积分为一：Integration to unity

![](/static/2020-11-24-17-47-13.png)

**一个概率质量函数或概率密度函数的总和/总和必须恰好是1**，

![](/static/2020-11-24-17-54-46.png)

* 因为所考虑的随机变量必须取某个值；
* 这是单位性的结果
* 一个实验的每一次重复都有正好一个结果

## PMF例子-掷骰子总和：PMF Example: sum of dice rolls

一个非常简单的离散PMF（概率分布）是两个六面骰子之和的期望值

* `P(X=x)=fX(x)`
* 对每个可能的结果都取值 x∈{2,3,4,5,6,7,8,9,10,11,12}

![](/static/2020-11-24-17-55-52.png)
![](/static/2020-11-24-17-56-12.png)

## 期望值：Expectation

![](/static/2020-11-24-18-00-14.png)

如果一个随机变量取数值，那么我们可以将**随机变量的期望值`E[X]`**定义为

* ![](/static/2020-11-24-18-01-53.png)

对于一个**概率质量函数 `P(X=x)=fX(x)` 的离散型随机变量**，我们将其写成一个和

* ![](/static/2020-11-24-18-03-41.png)

如果只有有限的几种可能，那么就是

* ![](/static/2020-11-24-18-04-05.png)

🍊 期望值是一个随机变量的"平均值"。

* 在非正式的情况下，它代表了我们"期望发生的事情"；
* 最有可能的总体"分数"
* 它可以被认为是一个实验的所有可能结果的加权和，其中每个结果是由该结果发生的概率加权的

---

![](/static/2020-11-24-18-05-23.png)

例如，在"一对骰子"的情况下，我们可以计算掷骰子后显示的"点"数的总期望值。

* 我们计算出每一个"点"显示的概率（**每个结果的概率**），然后乘以该"点"的数量（**每个结果本身**），再将结果相加。
* **这就是平均显示的"点"的期望值，或者说期望值**

---

想象一下，你遇到一个街头骗子，他请你玩两个骰子游戏。他给你提供了一个8英镑买进的机会；你扔完骰子后，骰子的上端面显示多少英镑，你就赢多少英镑。这是个公平的游戏吗？

不，预期收益只有7英镑，你必须投入8英镑才能玩，所以预期结果是亏损1英镑（有时会用"**负预期值negative expected value**"或`-ve EV`来表示）

* 如果是7英镑买进，游戏就很公平，因为平均你和老千不会转钱

## 期望和方法：Expectation and means

![](/static/2020-11-24-18-10-26.png)

期望值对应于mean或avertage结果的概念

* 一个随机变量的期望值是所有结果值的**真实平均值**，
  * 如果我们无限次地进行实验，就会观察到这些结果。
  * 这就是**群体平均值population mean** -- 一个随机变量的整体，可能是无穷大的总体的平均值

🍊 随机变量的许多重要性质可以用期望来定义

* 一个随机变量 `X` 的平均值就是 `E[X]`
  * 它是**中心倾向central tendency**的量度
* 一个随机变量 `X` 的方差是`var(X)=E[(X−E[X])^2`
  * 它是一个**传播spread**的度量

## 随机变量X的任意函数的期望值: Expectations of functions of X

我们可以将函数应用于随机变量，例如，随机变量的平方

![](/static/2020-11-24-18-17-45.png)

🍊 例子 - 可以计算以下简单期望

![](/static/2020-11-24-18-20-05.png)

也就是说，我们只需取每个结果的和/积分，通过函数`g(x)` ，按结果`x`的概率加权

* g(x)可以被认为是一个 "打分 "函数，它给X的每个结果分配一个实数。
* 请注意 `g(x)` 对**概率密度/质量函数（PDF/PMF） `fX(x)` 没有影响**
  * **它不影响结果的概率，只影响分配给结果的值**。
  * 例如，如果我们玩一个骰子游戏，投掷的分数是显示的点数的平方，（所以显示2个小点值4分，显示8个小点值64分，等等），我们将计算E[X^2]

---

🍬 注意 `E[f(X)]≠f(E[X])`

* 要计算一个函数的期望值，我们需要**计算每个结果的函数，并按该结果的概率加权求和**
  * 我们不能直接把函数应用到期望的结果上

或者我们可以计算一个双骰子游戏的期望值，在这个游戏中，任何一对相同的掷骰子都能得到10分，否则就是0分（"对子或无"）。

![](/static/2020-11-24-18-29-09.png)

* 将把它写成一个矩阵（roll`1`&`2`），以便于查看
* 这不会改变计算方法

🍊 预期值是做出**理性决策 rational decision**的关键，是**决策理论decision theory**的核心问题

* 它们将分数（或效用utility）与不确定性（概率）结合起来

例如，预期值为我们提供了一种决定玩骰子游戏值得支付多少钱的方法。如果单位是英镑，如果我们支付2.33英镑来玩这个游戏，我们就会收支平衡，如果我们支付2.00英镑来玩这个游戏，就会有利润（平均）。预期的平均利润只是我们玩每一局游戏所付的赌注减去一局游戏的预期值`E[game]=stake−E[X]`

## 样本&抽样：Samples and sampling

![](/static/2020-11-24-18-35-52.png)

🍊 **样本**

* **是实验的观察结果**
* 我们将同义使用观察这一术语，尽管样本通常指的是模拟，观察指的是具体的真实数据。

🍊 我们可以从一个**分布中取样**，

* 这意味着根据这些变量的概率分布来模拟结果。
* 我们也可以观察来自外部的数据，数据可能是由某个概率分布产生的。
  * 例如，我们可以通过掷两个骰子，并将结果相加，从骰子PMF的总和中取样。这就是从这个分布中取样或抽样
  * 对于离散型随机变量，这很容易：**我们只需根据每个结果的概率进行抽样就可以产生样本**。我们将在下一单元讨论抽样策略

## 经验分布：The empirical distribution

![](/static/2020-11-24-18-40-25.png)

对于离散数据，我们可以通过以下方式**估计可能产生观测值的概率质量函数PMF** -- 经验分布

* 算**每个观察结果/试验的总次数**
* 这就是所谓的经验分布

🍊 这可以被认为是结果发生次数的标准化直方图

![](/static/2020-11-24-18-42-41.png)

### 计算经验分布：Compute the Empirical Distribution

![](/static/2020-11-24-18-43-49.png)

对于**离散型随机变量**，**我们总是可以从一系列的观测值中计算出经验分布**；

* 例如，从文本语料库中（例如，1994年印刷的每篇报纸文章中）某个特定单词的计数中计算出
  * 我们只需计算**每个单词的出现次数/单词总数**

🍊 经验分布公式

![](/static/2020-11-24-18-45-57.png)

注意，经验分布是一个近似于未知真分布的分布。

* 对于非常大的离散变量样本，**经验分布将越来越接近真正的 PMF**，假设我们看到的样本是在一个无偏的方式绘制。
* <font color="red">然而，这种方法对于连续随机变量不起作用，因为我们只能看到一次观察到的值(想想为什么!).</font>

## 随机抽样程序：Random sampling procedures

### 统一抽样：Uniform sampling

![](/static/2020-11-24-18-48-07.png)

有一些算法可以产生连续的随机数，这些随机数在一个区间内均匀分布，比如从0.0到1.0

* 这些实际上是伪随机数，因为计算机是（希望）确定性的。
* 它们被设计成近似于真随机序列的统计特性。
* 从本质上讲，所有这类发生器都会产生离散符号（比特或整数）序列，然后将其映射到特定范围内的浮点数；这一点是相当棘手的

> 我们必须注意：**计算机生成的是伪随机浮点数，而不是真正的随机实数**。虽然这在很多时候没有什么区别，但它们是完全不同的东西

---

![](/static/2020-11-24-18-49-05.png)

**一个均匀分布的数字在其区间内取任何值的概率相等，其他地方的概率为零**。

* 虽然这是从一个连续的PDF中取样，但它是从任意PMF中取样的关键构件。
* 均匀分布的符号是 `X∼U(a,b)` ，
  * 这意味着 **`X` 是一个随机变量**，
  * **它可以在 `a` 和 `b` 之间取值，在这个区间内的任何数字都有相同的可能性**
  * 符号`~`应为"分布为"，即"`X`在[a,b]区间内为均匀分布"

> 请注意，在实践中，如果我们使用的是浮点，这些值在给定区间内是不统一的，因为我们只能对有效的浮点值进行采样。**虽然浮点数是非均匀分布的，但对于大多数应用来说，这种差异并不重要**

---

例如，我们知道NumPy函数`np.random.uniform(a,b,[shape])`

![](/static/2020-11-24-18-52-05.png)

* 用a和b之间的"连续"随机数填充一个数组

### 离散抽样：Discrete sampling

对于一个离散的概率质量函数PMF，我们可以根据任意的PMF，通过对单位区间的分割来对结果进行采样。

* 这就像在墙上贴海报，面积与每个结果的概率成正比，然后向墙上扔飞镖

![](/static/2020-11-24-18-53-36.png)

算法

* 选择任意排序的结果`x1,x2,…`
* 给每个结果分配一个"bin分组"，
  * **这个bin是区间`[0,1]`中与其概率相等的部分**，
  * 因此该区间被划分为**连续的非重叠区域** [P(x1)→P(x1)+P(x2),P(x1)+P(x2)→P(x1)+P(x2)+P(x3),…]
* 在`[0,1]`范围内均匀抽样
  * 无论它落在哪个"结果仓bin"，都是要抽取的样本

🍬 根据 PMF 的定义，所有概率的和将是1.0，所以它将完美地填充区间[0,1] ，没有空隙

![](/static/2020-11-24-18-58-35.png)
![](/static/2020-11-24-18-58-44.png)
![](/static/2020-11-24-18-58-51.png)

## 联合概率, 条件概率，边缘概率：Joint, conditional, marginal

### 联合概率：Joint Probability

![](/static/2020-11-24-19-01-52.png)

两个随机变量的联合概率写成`P(X,Y)`

* 给出了 `X` 和 `Y`同时取具体值的概率
  * `P(X=x)∧P(Y=y)`

### 边缘概率：Marginal Probability

![](/static/2020-11-24-19-04-19.png)

边际概率是指 由P(X,Y) 通过对 Y 所有可能的结果（求和）进行，对`P(X)`的积分推导出来的

* 这使得我们可以通过对涉及的另一个变量的所有可能结果求和，**从联合分布中计算出一个随机变量的分布**

![](/static/2020-11-24-19-08-49.png)

* 边缘化只是指对一个联合分布中的一个或多个变量进行整合：
  * 它将这些变量从分布中移除
* 如果两个随机变量之间没有任何依赖性，那么它们就是**独立的**。
  * 如果是这种情况，那么**联合分布只是单个分布的乘积**： `P(X,Y)=P(X)P(Y)`
  * <font color="red">这在变量有依赖性的一般情况下是不正确的</font>

### 条件概率：Conditional Probability

![](/static/2020-11-24-19-10-41.png)
![](/static/2020-11-24-19-11-57.png)

一个随机变量 X 给定随机变量 Y 的条件概率写成`P(X|Y)`

* 这告诉我们，如果我们已经知道（或固定）`Y` 的结果，那么 `X` 的结果的可能性有多大。
  * 读作"在`Y`已经取值`y`的情况下，`X`取值`x`时的概率"
  * **如果 X 和 Y 是独立的**，则条件概率为 `P(X|Y)=P(X)` ， `P(Y|X)=P(Y)`

## 二元模型：Bigrams

![](/static/2020-11-24-19-15-26.png)
![](/static/2020-11-24-19-18-19.png)
![](/static/2020-11-24-19-18-34.png)
![](/static/2020-11-24-19-18-47.png)
![](/static/2020-11-24-19-19-38.png)

## Odds & Log Odds（Logit）

odds

![](/static/2020-11-24-19-24-58.png)

* 不发生的概率/事件发生的概率

log-odds/Logit

![](/static/2020-11-24-19-27-21.png)

* 用的更多

## 多独立随机变量的概率：Log Probability

![](/static/2020-11-24-19-30-11.png)

### 对数可能性：log likelihoods

![](/static/2020-11-24-19-35-09.png)

### 比较对数可能性：compare log-likelihoods

![](/static/2020-11-24-19-37-33.png)

## 先验-可能-后验：Prior, likelihood, posterior

![](/static/2020-11-24-19-39-52.png)

### Invert the Probability Distribution

![](/static/2020-11-24-19-40-21.png)

### 贝叶斯规则术语: Nomenclature

![](/static/2020-11-24-19-46-41.png)

### 对证据的积分：Integration over the evidence

![](/static/2020-11-24-19-58-26.png)
![](/static/2020-11-24-19-59-42.png)

### Natural Frequency

![](/static/2020-11-24-20-00-28.png)

### 合并证据的贝叶斯规则：Bayes' rule for combining evidence

![](/static/2020-11-24-20-01-05.png)

## 熵：Entropy

![](/static/2020-11-24-20-03-14.png)

### 熵就是对数概率的期望值：Entropy is just the expectation of log-probability

![](/static/2020-11-24-20-04-08.png)

* 对数概率的平均值

### 投硬币：Tossing coins

![](/static/2020-11-24-20-07-30.png)
![](/static/2020-11-24-20-08-12.png)
![](/static/2020-11-24-20-08-29.png)

### 解读熵

![](/static/2020-11-24-20-08-49.png)